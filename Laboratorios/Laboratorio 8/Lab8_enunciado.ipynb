{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SDZmp2BO9KnQ"
      },
      "source": [
        "# **Laboratorio 8: Ready, Set, Deploy! üë©‚ÄçüöÄüë®‚ÄçüöÄ**\n",
        "\n",
        "<center><strong>MDS7202: Laboratorio de Programaci√≥n Cient√≠fica para Ciencia de Datos - Primavera 2025</strong></center>\n",
        "\n",
        "### Cuerpo Docente:\n",
        "\n",
        "- Profesores: Diego Cortez, Gabriel Iturra\n",
        "- Auxiliares: Melanie Pe√±a, Valentina Rojas\n",
        "- Ayudantes: Nicol√°s Cabello, Cristopher Urbina"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FdGqUgwX9pGQ"
      },
      "source": [
        "### Equipo: SUPER IMPORTANTE - notebooks sin nombre no ser√°n revisados\n",
        "\n",
        "- Nombre de alumno 1: Diego Acu√±a\n",
        "- Nombre de alumno 2: Tom√°s Ram√≠rez\n",
        "\n",
        "### **Link de repositorio de GitHub:** [Insertar Repositorio](https://github.com/Diego-Acuna/mds-laboratorios.git)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YraSOKrf9yMl"
      },
      "source": [
        "## Temas a tratar\n",
        "\n",
        "- Entrenamiento y registro de modelos usando MLFlow.\n",
        "- Despliegue de modelo usando FastAPI\n",
        "- Containerizaci√≥n del proyecto usando Docker\n",
        "\n",
        "## Reglas:\n",
        "\n",
        "- **Grupos de 2 personas**\n",
        "- Fecha de entrega: Entregas Martes a las 23:59.\n",
        "- Instrucciones del lab el viernes a las 16:15 en formato online. Asistencia no es obligatoria, pero se recomienda **fuertemente** asistir.\n",
        "- <u>Prohibidas las copias</u>. Cualquier intento de copia ser√° debidamente penalizado con el reglamento de la escuela.\n",
        "- Tienen que subir el laboratorio a u-cursos y a su repositorio de github. Labs que no est√©n en u-cursos no ser√°n revisados. Recuerden que el repositorio tambi√©n tiene nota.\n",
        "- Cualquier duda fuera del horario de clases al foro. Mensajes al equipo docente ser√°n respondidos por este medio.\n",
        "- Pueden usar cualquier material del curso que estimen conveniente.\n",
        "\n",
        "### Objetivos principales del laboratorio\n",
        "\n",
        "- Generar una soluci√≥n a un problema a partir de ML\n",
        "- Desplegar su soluci√≥n usando MLFlow, FastAPI y Docker\n",
        "\n",
        "El laboratorio deber√° ser desarrollado sin el uso indiscriminado de iteradores nativos de python (aka \"for\", \"while\"). La idea es que aprendan a exprimir al m√°ximo las funciones optimizadas que nos entrega `pandas`, las cuales vale mencionar, son bastante m√°s eficientes que los iteradores nativos sobre DataFrames."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "D98okEzUE8hb"
      },
      "source": [
        "# **Introducci√≥n**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lSiuBfGiFlQM"
      },
      "source": [
        "<p align=\"center\">\n",
        "  <img src=\"https://media.giphy.com/media/v1.Y2lkPTc5MGI3NjExODJnMHJzNzlkNmQweXoyY3ltbnZ2ZDlxY2c0aW5jcHNzeDNtOXBsdCZlcD12MV9pbnRlcm5hbF9naWZfYnlfaWQmY3Q9Zw/AbPdhwsMgjMjax5reo/giphy.gif\" width=\"400\">\n",
        "</p>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wPn8R-6u877j"
      },
      "source": [
        "\n",
        "\n",
        "Consumida en la tristeza el despido de Renac√≠n, Smapina ha deca√≠do en su desempe√±o, lo que se ha traducido en un irregular tratamiento del agua. Esto ha implicado una baja en la calidad del agua, llegando a haber algunos puntos de la comuna en la que el vital elemento no es apto para el consumo humano. Es por esto que la sanitaria p√∫blica de la municipalidad de Maip√∫ se ha contactado con ustedes para que le entreguen una urgente soluci√≥n a este problema (a la vez que dejan a Smapina, al igual que Renac√≠n, sin trabajo üòî).\n",
        "\n",
        "El problema que la empresa le ha solicitado resolver es el de elaborar un sistema que les permita saber si el agua es potable o no. Para esto, la sanitaria les ha proveido una base de datos con la lectura de m√∫ltiples sensores IOT colocados en diversas ca√±er√≠as, conductos y estanques. Estos sensores se√±alan nueve tipos de mediciones qu√≠micas y m√°s una etiqueta elaborada en laboratorio que indica si el agua es potable o no el agua.\n",
        "\n",
        "La idea final es que puedan, en el caso que el agua no sea potable, dar un aviso inmediato para corregir el problema. Tenga en cuenta que parte del equipo docente vive en Maip√∫ y su intoxicaci√≥n podr√≠a implicar graves problemas para el cierre del curso.\n",
        "\n",
        "Atributos:\n",
        "\n",
        "1. pH value\n",
        "2. Hardness\n",
        "3. Solids (Total dissolved solids - TDS)\n",
        "4. Chloramines\n",
        "5. Sulfate\n",
        "6. Conductivity\n",
        "7. Organic_carbon\n",
        "8. Trihalomethanes\n",
        "9. Turbidity\n",
        "\n",
        "Variable a predecir:\n",
        "\n",
        "10. Potability (1 si es potable, 0 no potable)\n",
        "\n",
        "Descripci√≥n de cada atributo se pueden encontrar en el siguiente link: [dataset](https://www.kaggle.com/adityakadiwal/water-potability)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-aIr6KegWsjS"
      },
      "source": [
        "# **1. Optimizaci√≥n de modelos con Optuna + MLFlow (2.0 puntos)**\n",
        "\n",
        "El objetivo de esta secci√≥n es que ustedes puedan combinar Optuna con MLFlow para poder realizar la optimizaci√≥n de los hiperpar√°metros de sus modelos.\n",
        "\n",
        "Como a√∫n no hemos hablado nada sobre `MLFlow` cabe preguntarse: **¬°¬øQu√© !\"#@ es `MLflow`?!**\n",
        "\n",
        "<p align=\"center\">\n",
        "  <img src=\"https://media.tenor.com/eusgDKT4smQAAAAC/matthew-perry-chandler-bing.gif\" width=\"400\">\n",
        "</p>\n",
        "\n",
        "## **MLFlow**\n",
        "\n",
        "`MLflow` es una plataforma de c√≥digo abierto que simplifica la gesti√≥n y seguimiento de proyectos de aprendizaje autom√°tico. Con sus herramientas, los desarrolladores pueden organizar, rastrear y comparar experimentos, adem√°s de registrar modelos y controlar versiones.\n",
        "\n",
        "<p align=\"center\">\n",
        "  <img src=\"https://spark.apache.org/images/mlflow-logo.png\" width=\"350\">\n",
        "</p>\n",
        "\n",
        "Si bien esta plataforma cuenta con un gran n√∫mero de herramientas y funcionalidades, en este laboratorio trabajaremos con dos:\n",
        "1. **Runs**: Registro que constituye la informaci√≥n guardada tras la ejecuci√≥n de un entrenamiento. Cada `run` tiene su propio run_id, el cual sirve como identificador para el entrenamiento en s√≠ mismo. Dentro de cada `run` podremos acceder a informaci√≥n como los hiperpar√°metros utilizados, las m√©tricas obtenidas, las librer√≠as requeridas y hasta nos permite descargar el modelo entrenado.\n",
        "2. **Experiments**: Se utilizan para agrupar y organizar diferentes ejecuciones de modelos (`runs`). En ese sentido, un experimento puede agrupar 1 o m√°s `runs`. De esta manera, es posible tambi√©n registrar m√©tricas, par√°metros y archivos (artefactos) asociados a cada experimento.\n",
        "\n",
        "### **Todo bien pero entonces, ¬øc√≥mo se usa en la pr√°ctica `MLflow`?**\n",
        "\n",
        "Es sencillo! Considerando un problema de machine learning gen√©rico, podemos registrar la informaci√≥n relevante del entrenamiento ejecutando `mlflow.autolog()` antes entrenar nuestro modelo. Veamos este bonito ejemplo facilitado por los mismos creadores de `MLflow`:\n",
        "\n",
        "```python\n",
        "#!pip install mlflow\n",
        "import mlflow # importar mlflow\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.datasets import load_diabetes\n",
        "from sklearn.ensemble import RandomForestRegressor\n",
        "\n",
        "db = load_diabetes()\n",
        "X_train, X_test, y_train, y_test = train_test_split(db.data, db.target)\n",
        "\n",
        "# Create and train models.\n",
        "rf = RandomForestRegressor(n_estimators=100, max_depth=6, max_features=3)\n",
        "\n",
        "mlflow.autolog() # registrar autom√°ticamente informaci√≥n del entrenamiento\n",
        "with mlflow.start_run(): #¬†delimita inicio y fin del run\n",
        "    #¬†aqu√≠ comienza el run\n",
        "    rf.fit(X_train, y_train) # train the model\n",
        "    predictions = rf.predict(X_test) # Use the model to make predictions on the test dataset.\n",
        "    # aqu√≠ termina el run\n",
        "```\n",
        "\n",
        "Si ustedes ejecutan el c√≥digo anterior en sus m√°quinas locales (desde un jupyter notebook por ejemplo) se dar√°n cuenta que en su directorio *root* se ha creado la carpeta `mlruns`. Esta carpeta lleva el tracking de todos los entrenamientos ejecutados desde el directorio root (importante: si se cambian de directorio y vuelven a ejecutar el c√≥digo anterior, se crear√° otra carpeta y no tendr√°n acceso al entrenamiento anterior). Para visualizar estos entrenamientos, `MLflow` nos facilita hermosa interfaz visual a la que podemos acceder ejecutando:\n",
        "\n",
        "```\n",
        "mlflow ui\n",
        "```\n",
        "\n",
        "y luego pinchando en la ruta http://127.0.0.1:5000 que nos retorna la terminal. Veamos en vivo algunas de sus funcionalidades!\n",
        "\n",
        "<p align=\"center\">\n",
        "  <img src=\"https://media4.giphy.com/media/v1.Y2lkPTc5MGI3NjExZXVuM3A5MW1heDFpa21qbGlwN2pyc2VoNnZsMmRzODZxdnluemo2bCZlcD12MV9pbnRlcm5hbF9naWZfYnlfaWQmY3Q9Zw/3o84sq21TxDH6PyYms/giphy.gif\" width=\"400\">\n",
        "</p>\n",
        "\n",
        "Les dejamos tambi√©n algunos comandos √∫tiles:\n",
        "\n",
        "- `mlflow.create_experiment(\"nombre_experimento\")`: Les permite crear un nuevo experimento para agrupar entrenamientos\n",
        "- `mlflow.log_metric(\"nombre_m√©trica\", m√©trica)`: Les permite registrar una m√©trica *custom* bajo el nombre de \"nombre_m√©trica\"\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ptP_ygr7S04t"
      },
      "source": [
        "## **1.1 Combinando Optuna + MLflow (2.0 puntos)**\n",
        "\n",
        "Ahora que tenemos conocimiento de ambas herramientas, intentemos ahora combinarlas para **m√°s sabor**. El objetivo de este apartado es simple: automatizar la optimizaci√≥n de los par√°metros de nuestros modelos usando `Optuna` y registrando de forma autom√°tica cada resultado en `MLFlow`.\n",
        "\n",
        "Considerando el objetivo planteado, se le pide completar la funci√≥n `optimize_model`, la cual debe:\n",
        "- **Optimizar los hiperpar√°metros del modelo `XGBoost` usando `Optuna`.**\n",
        "- **Registrar cada entrenamiento en un experimento nuevo**, asegur√°ndose de que la m√©trica `f1-score` se registre como `\"valid_f1\"`. No se deben guardar todos los experimentos en *Default*; en su lugar, cada `experiment` y `run` deben tener nombres interpretables, reconocibles y diferentes a los nombres por defecto (por ejemplo, para un run: \"XGBoost con lr 0.1\").\n",
        "- **Guardar los gr√°ficos de Optuna** dentro de una carpeta de artefactos de Mlflow llamada `/plots`.\n",
        "- **Devolver el mejor modelo** usando la funci√≥n `get_best_model` y serializarlo en el disco con `pickle.dump`. Luego, guardar el modelo en la carpeta `/models`.\n",
        "- **Guardar el c√≥digo en `optimize.py`**. La ejecuci√≥n de `python optimize.py` deber√≠a ejecutar la funci√≥n `optimize_model`.\n",
        "- **Guardar las versiones de las librer√≠as utilizadas** en el desarrollo.\n",
        "- **Respalde las configuraciones del modelo final y la importancia de las variables** en un gr√°fico dentro de la carpeta `/plots` creada anteriormente.\n",
        "\n",
        "*Hint: Le puede ser √∫til revisar los par√°metros que recibe `mlflow.start_run`*\n",
        "\n",
        "```python\n",
        "def get_best_model(experiment_id):\n",
        "    runs = mlflow.search_runs(experiment_id)\n",
        "    best_model_id = runs.sort_values(\"metrics.valid_f1\")[\"run_id\"].iloc[0]\n",
        "    best_model = mlflow.sklearn.load_model(\"runs:/\" + best_model_id + \"/model\")\n",
        "\n",
        "    return best_model\n",
        "```"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FTNLPUnm8yzD"
      },
      "outputs": [],
      "source": [
        "# Importamos las librer√≠as necesarias\n",
        "import os\n",
        "import json\n",
        "import pickle\n",
        "from datetime import datetime\n",
        "\n",
        "import optuna\n",
        "from optuna.samplers import TPESampler\n",
        "import plotly.io as pio\n",
        "\n",
        "import xgboost as xgb\n",
        "import mlflow\n",
        "import mlflow.sklearn\n",
        "import matplotlib.pyplot as plt\n",
        "import pandas as pd\n",
        "import sklearn\n",
        "from sklearn.metrics import f1_score\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# Definimos una funci√≥n para recuperar el mejor modelo\n",
        "def get_best_model(experiment_name: str):\n",
        "    exp_local = mlflow.get_experiment_by_name(experiment_name)\n",
        "    if exp_local is None:\n",
        "        raise RuntimeError(f\"Experimento '{experiment_name}' no encontrado en MLflow\")\n",
        "    runs = mlflow.search_runs(experiment_ids=[exp_local.experiment_id])\n",
        "    if runs.empty or \"metrics.valid_f1\" not in runs.columns:\n",
        "        raise RuntimeError(\"No se encontraron runs con la m√©trica 'valid_f1' en el experimento\")\n",
        "    best_run_id = runs.sort_values(\"metrics.valid_f1\", ascending=False)[\"run_id\"].iloc[0]\n",
        "    return mlflow.sklearn.load_model(f\"runs:/{best_run_id}/model\")\n",
        "\n",
        "def optimize_model(n_trials: int = 50, random_state: int = 42):\n",
        "    \"\"\"\n",
        "    1) Optimiza hiperpar√°metros de XGBoost usando Optuna.\n",
        "    2) Registra cada trial como un run dentro de un experimento nuevo (nombres interpretables).\n",
        "       Cada run registra la m√©trica 'valid_f1' y guarda el modelo con mlflow.sklearn.log_model.\n",
        "    3) Guarda los gr√°ficos de Optuna en plots/ y los sube a MLflow como artefactos.\n",
        "    4) Recupera el mejor modelo (get_best_model), lo serializa con pickle en models/model.pkl\n",
        "       y guarda ese pickle en los artefactos MLflow en 'models'.\n",
        "    5) Registra versiones de librer√≠as y guarda configuraci√≥n final y la importancia de variables\n",
        "       en plots/.\n",
        "    Devuelve (experiment_name, best_model_loaded).\n",
        "    \"\"\"\n",
        "\n",
        "    # Cargamos los datos\n",
        "    df = pd.read_csv(\"water_potability.csv\")\n",
        "    X = df.drop(\"Potability\", axis=1)\n",
        "    y = df[\"Potability\"]\n",
        "\n",
        "    # Separamos los datos en train, val y test\n",
        "    X_train_val, X_test, y_train_val, y_test = train_test_split(\n",
        "        X, y, test_size=0.2, random_state=random_state, stratify=y\n",
        "    )\n",
        "    X_train, X_val, y_train, y_val = train_test_split(\n",
        "        X_train_val, y_train_val, test_size=0.2, random_state=random_state, stratify=y_train_val\n",
        "    )\n",
        "\n",
        "    # Creamos un nuevo experimento en MLflow\n",
        "    timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
        "    experiment_name = f\"XGBoost_Optuna_{timestamp}\"\n",
        "    mlflow.set_experiment(experiment_name)\n",
        "    exp = mlflow.get_experiment_by_name(experiment_name)\n",
        "\n",
        "    # carpetas locales para artefactos\n",
        "    os.makedirs(\"plots\", exist_ok=True)\n",
        "    os.makedirs(\"models\", exist_ok=True)\n",
        "\n",
        "    # Definimos la funci√≥n objetivo para Optuna\n",
        "    def objective(trial):\n",
        "        # Definimos el espacio de b√∫squeda de hiperpar√°metros\n",
        "        params = {\n",
        "            \"max_depth\": trial.suggest_int(\"max_depth\", 3, 12),\n",
        "            \"learning_rate\": trial.suggest_float(\"learning_rate\", 0.01, 0.3, log=True),\n",
        "            \"n_estimators\": trial.suggest_int(\"n_estimators\", 50, 400),\n",
        "            \"subsample\": trial.suggest_float(\"subsample\", 0.6, 1.0),\n",
        "            \"colsample_bytree\": trial.suggest_float(\"colsample_bytree\", 0.6, 1.0),\n",
        "            \"gamma\": trial.suggest_float(\"gamma\", 0.0, 5.0),\n",
        "            \"min_child_weight\": trial.suggest_int(\"min_child_weight\", 1, 10),\n",
        "        }\n",
        "\n",
        "        # Definimos el nombre de la run\n",
        "        run_name = f\"XGBoost_lr{params['learning_rate']:.3f}_md{params['max_depth']}_n{params['n_estimators']}\"\n",
        "        \n",
        "        # Iniciamos una nueva run en MLflow\n",
        "        with mlflow.start_run(run_name=run_name):\n",
        "            # Registramos los hiperpar√°metros\n",
        "            mlflow.log_params(params)\n",
        "\n",
        "            # Entrenamos el modelo\n",
        "            clf = xgb.XGBClassifier(use_label_encoder=False, eval_metric=\"logloss\", **params)\n",
        "            clf.fit(\n",
        "                X_train,\n",
        "                y_train,\n",
        "                eval_set=[(X_val, y_val)],\n",
        "                verbose=False,\n",
        "            )\n",
        "\n",
        "            preds = clf.predict(X_val)\n",
        "            f1 = float(f1_score(y_val, preds))\n",
        "\n",
        "            # Registramos la m√©trica de validaci√≥n con el nombre requerido\n",
        "            mlflow.log_metric(\"valid_f1\", f1)\n",
        "\n",
        "            # Registramos modelo en esta run para poder recuperarlo m√°s tarde con runs:/<id>/model\n",
        "            mlflow.sklearn.log_model(clf, artifact_path=\"model\")\n",
        "\n",
        "            # Guardamos la informaci√≥n del trial en un archivo JSON en plots/ tal como se pide\n",
        "            trial_info = {\"trial_number\": trial.number, \"valid_f1\": f1, \"params\": params}\n",
        "            trial_fn = f\"plots/trial_{trial.number:03d}_info.json\"\n",
        "            with open(trial_fn, \"w\") as fh:\n",
        "                json.dump(trial_info, fh)\n",
        "            mlflow.log_artifact(trial_fn, artifact_path=\"plots\")\n",
        "\n",
        "        return f1\n",
        "\n",
        "    # Ejecutamos la optimizaci√≥n con Optuna\n",
        "    study = optuna.create_study(direction=\"maximize\", sampler=TPESampler(seed=random_state))\n",
        "    study.optimize(objective, n_trials=n_trials, n_jobs=1)\n",
        "\n",
        "    # Guardamos los gr√°ficos de Optuna en plots/ y los subimos a MLflow\n",
        "    fig_hist = optuna.visualization.plot_optimization_history(study)\n",
        "    pio.write_html(fig_hist, \"plots/optuna_history.html\", auto_open=False)\n",
        "\n",
        "    fig_param = optuna.visualization.plot_param_importances(study)\n",
        "    if fig_param is not None:\n",
        "        pio.write_html(fig_param, \"plots/optuna_param_importances.html\", auto_open=False)\n",
        "\n",
        "    # Guardamos los mejores hiperpar√°metros en un archivo JSON en plots/\n",
        "    best_params = study.best_params\n",
        "    with open(\"plots/best_params.json\", \"w\") as fh:\n",
        "        json.dump(best_params, fh)\n",
        "\n",
        "    # Recuperamos el mejor modelo usando la funci√≥n definida\n",
        "    best_model_loaded = get_best_model(experiment_name)\n",
        "\n",
        "    # Serializamos el modelo con pickle en models/model.pkl\n",
        "    model_pickle_path = \"models/model.pkl\"\n",
        "    with open(model_pickle_path, \"wb\") as fh:\n",
        "        pickle.dump(best_model_loaded, fh)\n",
        "\n",
        "    # Creamos una nueva run para guardar artefactos y metadata final\n",
        "    final_run_name = f\"artifacts_and_metadata_{timestamp}\"\n",
        "    with mlflow.start_run(run_name=final_run_name):\n",
        "        # Guardamos las versiones de las librer√≠as usadas\n",
        "        mlflow.log_param(\"xgboost_version\", xgb.__version__)\n",
        "        mlflow.log_param(\"optuna_version\", optuna.__version__)\n",
        "        mlflow.log_param(\"mlflow_version\", mlflow.__version__)\n",
        "        mlflow.log_param(\"sklearn_version\", sklearn.__version__)\n",
        "        mlflow.log_param(\"pandas_version\", pd.__version__)\n",
        "\n",
        "        # Subimos los gr√°ficos y archivos JSON generados en plots/\n",
        "        mlflow.log_artifact(\"plots/optuna_history.html\", artifact_path=\"plots\")\n",
        "        if os.path.exists(\"plots/optuna_param_importances.html\"):\n",
        "            mlflow.log_artifact(\"plots/optuna_param_importances.html\", artifact_path=\"plots\")\n",
        "        mlflow.log_artifact(\"plots/best_params.json\", artifact_path=\"plots\")\n",
        "        # subir todos los trial jsons generados\n",
        "        for fn in sorted(os.listdir(\"plots\")):\n",
        "            if fn.startswith(\"trial_\") and fn.endswith(\"_info.json\"):\n",
        "                mlflow.log_artifact(os.path.join(\"plots\", fn), artifact_path=\"plots\")\n",
        "\n",
        "        # Importamos la importancia de variables y la guardamos en plots/\n",
        "        try:\n",
        "            if hasattr(best_model_loaded, \"get_booster\"):\n",
        "                booster = best_model_loaded.get_booster()\n",
        "                ax = xgb.plot_importance(booster, max_num_features=20)\n",
        "            else:\n",
        "                ax = xgb.plot_importance(best_model_loaded, max_num_features=20)\n",
        "            plt.tight_layout()\n",
        "            imp_path = \"plots/importance_plot.png\"\n",
        "            plt.savefig(imp_path)\n",
        "            plt.close()\n",
        "            mlflow.log_artifact(imp_path, artifact_path=\"plots\")\n",
        "        except Exception:\n",
        "            pass\n",
        "\n",
        "        # Subimos el pickle del modelo\n",
        "        mlflow.log_artifact(model_pickle_path, artifact_path=\"models\")\n",
        "\n",
        "    # Retornamos el nombre del experimento y el mejor modelo cargado\n",
        "    print(f\"Experiment saved: {experiment_name} (id={exp.experiment_id})\")\n",
        "    return experiment_name, best_model_loaded"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tL2iG18289j9"
      },
      "source": [
        "# **2. FastAPI (2.0 puntos)**\n",
        "\n",
        "<div align=\"center\">\n",
        "  <img src=\"https://media3.giphy.com/media/YQitE4YNQNahy/giphy-downsized-large.gif\" width=\"500\">\n",
        "</div>\n",
        "\n",
        "Con el modelo ya entrenado, la idea de esta secci√≥n es generar una API REST a la cual se le pueda hacer *requests* para as√≠ interactuar con su modelo. En particular, se le pide:\n",
        "\n",
        "- Guardar el c√≥digo de esta secci√≥n en el archivo `main.py`. Note que ejecutar `python main.py` deber√≠a levantar el servidor en el puerto por defecto.\n",
        "- Defina `GET` con ruta tipo *home* que describa brevemente su modelo, el problema que intenta resolver, su entrada y salida.\n",
        "- Defina un `POST` a la ruta `/potabilidad/` donde utilice su mejor optimizado para predecir si una medici√≥n de agua es o no potable. Por ejemplo, una llamada de esta ruta con un *body*:\n",
        "\n",
        "```json\n",
        "{\n",
        "   \"ph\":10.316400384553162,\n",
        "   \"Hardness\":217.2668424334475,\n",
        "   \"Solids\":10676.508475429378,\n",
        "   \"Chloramines\":3.445514571005745,\n",
        "   \"Sulfate\":397.7549459751925,\n",
        "   \"Conductivity\":492.20647361771086,\n",
        "   \"Organic_carbon\":12.812732207582542,\n",
        "   \"Trihalomethanes\":72.28192021570328,\n",
        "   \"Turbidity\":3.4073494284238364\n",
        "}\n",
        "```\n",
        "\n",
        "Su servidor deber√≠a retornar una respuesta HTML con c√≥digo 200 con:\n",
        "\n",
        "\n",
        "```json\n",
        "{\n",
        "  \"potabilidad\": 0 # respuesta puede variar seg√∫n el clasificador que entrenen\n",
        "}\n",
        "```\n",
        "\n",
        "**`HINT:` Recuerde que puede utilizar [http://localhost:8000/docs](http://localhost:8000/docs) para hacer un `POST`.**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HSausqDJ9CQh"
      },
      "source": [
        "# **3. Docker (2 puntos)**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fNmC483flS00"
      },
      "source": [
        "<div align=\"center\">\n",
        "  <img src=\"https://miro.medium.com/v2/resize:fit:1400/1*9rafh2W0rbRJIKJzqYc8yA.gif\" width=\"500\">\n",
        "</div>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "niMA_qsCjqlv"
      },
      "source": [
        "Tras el √©xito de su aplicaci√≥n web para generar la salida, Smapina le solicita que genere un contenedor para poder ejecutarla en cualquier computador de la empresa de agua potable.\n",
        "\n",
        "## **3.1 Creaci√≥n de Container (1 punto)**\n",
        "\n",
        "Cree un Dockerfile que use una imagen base de Python, copie los archivos del proyecto e instale las dependencias desde un `requirements.txt`. Con esto, construya y ejecute el contenedor Docker para la API configurada anteriormente. Entregue el c√≥digo fuente (incluyendo `main.py`, `requirements.txt`, y `Dockerfile`) y la imagen Docker de la aplicaci√≥n. Para la dockerizaci√≥n, aseg√∫rese de cumplir con los siguientes puntos:\n",
        "\n",
        "1. **Generar un archivo `.dockerignore`** que ignore carpetas y archivos innecesarios dentro del contenedor.\n",
        "2. **Configurar un volumen** que permita la persistencia de los datos en una ruta local del computador.\n",
        "3. **Exponer el puerto** para acceder a la ruta de la API sin tener que entrar al contenedor directamente.\n",
        "4. **Incluir im√°genes en el notebook** que muestren la ejecuci√≥n del contenedor y los resultados obtenidos.\n",
        "5. **Revisar y comentar los recursos utilizados por el contenedor**. Analice si los contenedores son livianos en t√©rminos de recursos."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**Respuestas:**\n",
        "\n",
        "**Pregunta 4:** A continuaci√≥n se muestran unos pantallazos que muestran la ejecuci√≥n del contenedor y los resultados obtenidos. En primer lugar, se muestra una imagen con el estado del proyecto antes de inicializar la imagen del Docker de la aplicaci√≥n:\n",
        "<p align=\"center\">\n",
        "  <img src=\"images/00_project_tree.png\" width=\"400\">\n",
        "</p>\n",
        "A continuaci√≥n, se muestra una imagen del comando para construir el Docker junto al resultado obtenido:\n",
        "<p align=\"center\">\n",
        "  <img src=\"images/01_docker_build.png\" width=\"1000\">\n",
        "</p>\n",
        "Similarmente, se muestra el comando para obtener la imagen del Docker junto al resultado obtenido (que muestra la imagen y su tama√±o).\n",
        "<p align=\"center\">\n",
        "  <img src=\"images/02_docker_images.png\" width=\"1000\">\n",
        "</p>\n",
        "Por otro lado, se muestra el resultado obtenido al hacer la ejecuci√≥n del contenedor con el comando run.\n",
        "<p align=\"center\">\n",
        "  <img src=\"images/03_docker_run.png\" width=\"1000\">\n",
        "</p>\n",
        "De forma parecida a lo anterior, se muestran tambi√©n los contenedores en ejecuci√≥n con el comando docker ps.\n",
        "<p align=\"center\">\n",
        "  <img src=\"images/04_docker_ps.png\" width=\"1000\">\n",
        "</p>\n",
        "Tambi√©n se muestra a continuaci√≥n los logs del contenedor.\n",
        "<p align=\"center\">\n",
        "  <img src=\"images/05_docker_logs.png\" width=\"1000\">\n",
        "</p>\n",
        "Adem√°s de comandos como tal, tambi√©n se muestra a continuaci√≥n c√≥mo reacciona la API frente al input del enunciado.\n",
        "<p align=\"center\">\n",
        "  <img src=\"images/06_post_response.png\" width=\"1000\">\n",
        "</p>\n",
        "Finalmente, se muestra el uso de recursos para ver si el contendor es liviano o no y as√≠ poder responder la siguiente pregunta.\n",
        "<p align=\"center\">\n",
        "  <img src=\"images/07_docker_stats.png\" width=\"1000\">\n",
        "</p>\n",
        "\n",
        "**Pregunta 5:** En la revisi√≥n de los recursos utilizados por el contenedor, a partir del comando \"docker stats --no-stream potability_api\", se observa un consumo de CPU del 0.12 % y una utilizaci√≥n de memoria de aproximadamente 127.9 MiB, lo que representa solo un 1.6 % del total disponible. Estos valores evidencian que, una vez cargado el modelo y en funcionamiento el servidor Uvicorn, el contenedor mantiene un consumo muy bajo de recursos, utilizando procesamiento solo cuando recibe solicitudes. El uso de red y disco tambi√©n es m√≠nimo (unos pocos KB transferidos y cerca de 64 MB en operaciones de lectura, correspondientes principalmente a la carga del modelo y generaci√≥n de logs), y el n√∫mero de procesos activos (67 en total) es normal para un entorno Python que ejecuta servicios como FastAPI y XGBoost.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## **3.2 Preguntas de Smapina (1 punto)**\n",
        "Tras haber experimentado con Docker, Smapina desea profundizar m√°s en el tema y decide realizarle las siguientes consultas:\n",
        "\n",
        "1. ¬øC√≥mo se diferencia Docker de una m√°quina virtual (VM)?\n",
        "2. ¬øCu√°l es la diferencia entre usar Docker y ejecutar la aplicaci√≥n directamente en el sistema local?\n",
        "3. ¬øC√≥mo asegura Docker la consistencia entre diferentes entornos de desarrollo y producci√≥n?\n",
        "4. ¬øC√≥mo se gestionan los vol√∫menes en Docker para la persistencia de datos?\n",
        "5. ¬øQu√© son Dockerfile y docker-compose.yml, y cu√°l es su prop√≥sito?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**Respuestas:**\n",
        "\n",
        "**Pregunta 1:** Docker y las m√°quinas virtuales comparten la idea de aislar aplicaciones, pero lo hacen de forma muy distinta. Docker ofrece un aislamiento a nivel de proceso: los contenedores comparten el kernel del sistema operativo del host y utilizan espacios de nombres y control groups para mantener la separaci√≥n. En cambio, una m√°quina virtual a√≠sla a nivel de hardware, simulando componentes f√≠sicos y ejecutando un sistema operativo completo. Esta diferencia hace que Docker sea mucho m√°s liviano, ya que no requiere emular hardware ni cargar otro sistema operativo. Por lo mismo, los contenedores se inician en segundos, mientras que una m√°quina virtual puede tardar varios minutos. En t√©rminos de eficiencia, los contenedores usan menos recursos, lo que permite ejecutar m√°s servicios en la misma m√°quina f√≠sica, aunque las m√°quinas virtuales ofrecen un nivel de aislamiento m√°s fuerte y son m√°s adecuadas cuando se necesita seguridad extrema o diferentes kernels.\n",
        "\n",
        "**Pregunta 2:** Ejecutar una aplicaci√≥n dentro de Docker ofrece varias ventajas frente a hacerlo directamente en el entorno local. Cuando se usa Docker, todas las dependencias, versiones de librer√≠as, configuraciones y comandos necesarios quedan encapsulados en la imagen, lo que garantiza que el entorno sea exactamente el mismo en cualquier m√°quina donde se ejecute. Esto elimina los cl√°sicos problemas de compatibilidad entre sistemas operativos y versiones, asegurando que lo que funciona en un computador funcionar√° igual en otro. En cambio, al ejecutar la aplicaci√≥n localmente, el funcionamiento depende del entorno del usuario: versiones distintas de Python, librer√≠as instaladas globalmente o diferencias en la configuraci√≥n del sistema pueden provocar errores. Docker adem√°s proporciona aislamiento, evitando que los paquetes o dependencias de un proyecto interfieran con otros, y facilita la portabilidad, ya que basta con transferir la imagen y ejecutarla con los mismos comandos.\n",
        "\n",
        "**Pregunta 3:** Docker asegura la consistencia entre entornos mediante el uso de im√°genes inmutables, versionadas y reproducibles. Cada imagen se construye a partir de un Dockerfile, un documento que especifica paso a paso c√≥mo crear el entorno: desde la base del sistema operativo hasta la instalaci√≥n de dependencias y la configuraci√≥n final. De este modo, cualquier persona o servidor que ejecute el comando docker build a partir de ese archivo obtendr√° una imagen id√©ntica. Adem√°s, el uso de archivos como el archivo requirements.txt creado para este laboratorio, permite fijar las versiones exactas de las librer√≠as, asegurando que no haya diferencias entre desarrollo, prueba y producci√≥n. En resumen, Docker crea entornos totalmente replicables y previsibles, evitando los errores derivados de configuraciones distintas entre m√°quinas.\n",
        "\n",
        "**Pregunta 4:** Docker maneja la persistencia de datos a trav√©s de vol√∫menes, que permiten que la informaci√≥n generada dentro del contenedor no se pierda al detener o eliminarlo. Los vol√∫menes pueden ser de dos tipos: los bind mounts y los named volumes. En los primeros, se vincula una carpeta del sistema host con una ruta dentro del contenedor, lo que permite acceder directamente a los archivos desde ambos entornos. Los named volumes, en cambio, son gestionados por Docker y almacenan los datos de forma independiente del sistema de archivos del host, lo que es √∫til en despliegues m√°s controlados o compartidos. En ambos casos, los datos persisten incluso si el contenedor se elimina, garantizando que los resultados, modelos entrenados o configuraciones no se pierdan entre ejecuciones. Este mecanismo tambi√©n facilita la separaci√≥n entre el c√≥digo (que puede actualizarse libremente) y la informaci√≥n almacenada (que debe preservarse). En resumen, los vol√∫menes son una herramienta clave para asegurar la persistencia y la interoperabilidad entre el entorno de desarrollo y el contenedor.\n",
        "\n",
        "**Pregunta 5:** Dockerfile, por un lado, es la receta para construir una imagen. Define base (en el caso de este laboratorio python:3.11-slim), copia fuentes, instala dependencias, expone puertos y comando final (Uvicorn), obteniendo como resultado una imagen portable (en este caso, generamos una imagen de aproximadamente 3GB). \n",
        "\n",
        "Por otro lado, docker-compose.yml es una orquestaci√≥n en tiempo de ejecuci√≥n para uno o varios servicios (app, DB, broker). Declara vol√∫menes, puertos, redes, variables de forma declarativa y levanta todo con docker compose up -d."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8xJ_ZK1IfnZW"
      },
      "source": [
        "# Conclusi√≥n\n",
        "\n",
        "√âxito!\n",
        "<div align=\"center\">\n",
        "  <img src=\"https://i.pinimg.com/originals/55/f5/fd/55f5fdc9455989f8caf7fca7f93bd96a.gif\" width=\"500\">\n",
        "</div>"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.5"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
