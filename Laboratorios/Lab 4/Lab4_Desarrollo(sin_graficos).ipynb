{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "cell_id": "ac9155b9f5e04400957a6f8bb3f6610c",
        "deepnote_cell_type": "markdown",
        "id": "2v2D1coL7I8i"
      },
      "source": [
        "<h1><center>Laboratorio 4: La solicitud de Mathias ü§ó</center></h1>\n",
        "\n",
        "<center><strong>MDS7202: Laboratorio de Programaci√≥n Cient√≠fica para Ciencia de Datos - Primavera 2025</strong></center>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "cell_id": "d3d6f6d405c54dbe985a5f4b3e4f9120",
        "deepnote_cell_type": "markdown",
        "id": "YxdTmIPD7L_x"
      },
      "source": [
        "### Cuerpo Docente:\n",
        "\n",
        "- Profesores: Diego Cortez, Gabriel Iturra\n",
        "- Auxiliares: Melanie Pe√±a, Valentina Rojas\n",
        "- Ayudantes: Nicol√°s Cabello, Cristopher Urbina"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "cell_id": "851a7788e8214942863cbd4099064ab2",
        "deepnote_cell_type": "markdown",
        "id": "Y2Gyrj-x7N2L"
      },
      "source": [
        "### Equipo: SUPER IMPORTANTE - notebooks sin nombre no ser√°n revisados\n",
        "\n",
        "- Nombre de alumno 1: Diego Acu√±a\n",
        "- Nombre de alumno 2: Tom√°s Ram√≠rez\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "cell_id": "f23a189afdec4e198683308db70e43b7",
        "deepnote_cell_type": "markdown",
        "id": "jQ9skYc57Pxi"
      },
      "source": [
        "### **Link de repositorio de GitHub:** [Repositorio de GitHub](https://github.com/Diego-Acuna/mds-laboratorios)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "cell_id": "b5318f41cda64d4290a7a548956ed725",
        "deepnote_cell_type": "markdown",
        "id": "1M4PoEWm7S80"
      },
      "source": [
        "## Temas a tratar\n",
        "- Aplicar Pandas para obtener caracter√≠sticas de un DataFrame.\n",
        "- Aplicar Pipelines y Column Transformers.\n",
        "- Utilizar diferentes algoritmos de cluster y ver el desempe√±o.\n",
        "\n",
        "## Reglas:\n",
        "\n",
        "- **Grupos de 2 personas**\n",
        "- Fecha de entrega: Entregas Martes a las 23:59.\n",
        "- Instrucciones del lab el viernes a las 16:15 en formato online. Asistencia no es obligatoria, pero se recomienda **fuertemente** asistir.\n",
        "- <u>Prohibidas las copias</u>. Cualquier intento de copia ser√° debidamente penalizado con el reglamento de la escuela.\n",
        "- Tienen que subir el laboratorio a u-cursos y a su repositorio de github. Labs que no est√©n en u-cursos no ser√°n revisados. Recuerden que el repositorio tambi√©n tiene nota.\n",
        "- Cualquier duda fuera del horario de clases al foro. Mensajes al equipo docente ser√°n respondidos por este medio.\n",
        "- Pueden usar cualquier material del curso que estimen conveniente.\n",
        "\n",
        "### Objetivos principales del laboratorio\n",
        "- Comprender c√≥mo aplicar pipelines de Scikit-Learn para generar clusters.\n",
        "- Familiarizarse con plotly.\n",
        "\n",
        "El laboratorio deber√° ser desarrollado sin el uso indiscriminado de iteradores nativos de python (aka \"for\", \"while\"). La idea es que aprendan a exprimir al m√°ximo las funciones optimizadas que nos entrega `numpy`, las cuales vale mencionar, son bastante m√°s eficientes que los iteradores nativos sobre arreglos (*o tensores*)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "cell_id": "858df483d9e64780a21674afed1d34b8",
        "deepnote_cell_type": "markdown",
        "id": "SuMbiyQZG2Cc"
      },
      "source": [
        "## Descripci√≥n del laboratorio"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "cell_id": "403ffe48ec994afda4b91e670a08d0ef",
        "deepnote_cell_type": "markdown",
        "id": "QZsNO4rUrqCz"
      },
      "source": [
        "<center>\n",
        "<img src=\"https://i.pinimg.com/originals/5a/a6/af/5aa6afde8490da403a21601adf7a7240.gif\" width=400 />"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "cell_id": "0303baa17d4546feae8c9b88c58470bf",
        "deepnote_cell_type": "markdown",
        "id": "2o0MPuk8rqCz"
      },
      "source": [
        "En el coraz√≥n de las operaciones de Aerol√≠nea Lucero, Mathias, el gerente de an√°lisis de datos, reuni√≥ a un talentoso equipo de j√≥venes cient√≠ficos de datos para un desaf√≠o crucial: segmentar la base de datos de los clientes. ‚ÄúNuestro objetivo es descubrir patrones en el comportamiento de los pasajeros que nos permitan personalizar servicios y optimizar nuestras campa√±as de marketing,‚Äù explic√≥ Mathias, mientras desplegaba un amplio rango de datos que inclu√≠an desde h√°bitos de compra hasta opiniones sobre los vuelos.\n",
        "\n",
        "Mathias encarg√≥ a los cient√≠ficos de datos la tarea de aplicar t√©cnicas avanzadas de clustering para identificar distintos segmentos de clientes, como los viajeros frecuentes y aquellos que eligen la aerol√≠nea para celebrar ocasiones especiales. La meta principal era entender profundamente c√≥mo estos grupos perciben la calidad y satisfacci√≥n de los servicios ofrecidos por la aerol√≠nea.\n",
        "\n",
        "A trav√©s de un enfoque meticuloso y colaborativo, los cient√≠ficos de datos se abocaron a la tarea, buscando transformar los datos brutos en valiosos insights que permitir√≠an a Aerol√≠nea Lucero no solo mejorar su servicio, sino tambi√©n fortalecer las relaciones con sus clientes mediante una oferta m√°s personalizada y efectiva."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "cell_id": "e78cb41b144041af98928ab26dcfdaa9",
        "deepnote_cell_type": "markdown",
        "id": "hs4KKWF1Hdpo"
      },
      "source": [
        "## Importamos librerias utiles üò∏"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cell_id": "95a5533cfd6d49cfb9afc111c44d224f",
        "deepnote_cell_type": "code",
        "deepnote_to_be_reexecuted": false,
        "execution_millis": 15,
        "execution_start": 1714107106552,
        "id": "a4YpMafirqC0",
        "source_hash": null
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "from sklearn import datasets\n",
        "\n",
        "import plotly.express as px\n",
        "import plotly.graph_objects as go\n",
        "from plotly.subplots import make_subplots\n",
        "\n",
        "import time\n",
        "\n",
        "from sklearn.cluster import KMeans, AgglomerativeClustering, DBSCAN\n",
        "from sklearn.mixture import GaussianMixture\n",
        "from sklearn.compose import ColumnTransformer\n",
        "from sklearn.preprocessing import StandardScaler, RobustScaler, FunctionTransformer\n",
        "from sklearn.pipeline import Pipeline, make_pipeline\n",
        "from sklearn.metrics import silhouette_score\n",
        "from sklearn.decomposition import PCA\n",
        "from sklearn.ensemble import IsolationForest\n",
        "from sklearn.mixture import GaussianMixture\n",
        "from sklearn.base import clone\n",
        "from sklearn import datasets"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "cell_id": "acbeab32db6146678e75448dddf43da8",
        "deepnote_cell_type": "markdown",
        "id": "UQOXod4gHhSq"
      },
      "source": [
        "## 1. Estudio de Performance üìà [10 Puntos]\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "cell_id": "704b56b978254ad3ae12cdbf58f4832d",
        "deepnote_cell_type": "markdown",
        "id": "Gn5u5ICkrqC2"
      },
      "source": [
        "<center>\n",
        "<img src=\"https://i.pinimg.com/originals/23/b7/6e/23b76e9e77e63c0eec1a7b28372369e3.gif\" width=300>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "cell_id": "d35fbdcc5ef045d6a2822622f0714179",
        "deepnote_cell_type": "markdown",
        "id": "y4Z0jTjtrqC2"
      },
      "source": [
        "Don Mathias les ha encomendado su primera tarea: analizar diversas t√©cnicas de clustering. Su objetivo es entender detalladamente c√≥mo funcionan estos m√©todos en t√©rminos de segmentaci√≥n y eficiencia en tiempo de ejecuci√≥n.\n",
        "\n",
        "Analice y compare el desempe√±o, tiempo de ejecuci√≥n y visualizaciones de cuatro algoritmos de clustering (k-means, DBSCAN, Ward y GMM) aplicados a tres conjuntos de datos, incrementando progresivamente su tama√±o. Utilice Plotly para las gr√°ficas y discuta los resultados tanto cualitativa como cuantitativamente.\n",
        "\n",
        "Uno de los requisitos establecidos por Mathias es que el an√°lisis se lleve a cabo utilizando Plotly; de no ser as√≠, se considerar√° incorrecto. Para facilitar este proceso, se ha proporcionado un c√≥digo de Plotly que puede servir como base para realizar las gr√°ficas. Ap√≥yese en el c√≥digo entregado para efectuar el an√°lisis y tome como referencia la siguiente imagen para realizar los gr√°ficos:\n",
        "\n",
        "<img src='https://gitlab.com/imezadelajara/datos_clase_7_mds7202/-/raw/main/misc_images/Screenshot_2024-04-26_at_9.10.44_AM.png' width=800 />\n",
        "\n",
        "En el gr√°fico se visualizan en dos dimensiones los diferentes tipos de datos proporcionados en `datasets`. Cada columna corresponde a un modelo de clustering diferente, mientras que cada fila representa un conjunto de datos distinto. Cada uno de los gr√°ficos incluye el tiempo en segundos que tarda el an√°lisis y la m√©trica Silhouette obtenida."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "cell_id": "37580aab6cef4238a8ce42c50a6d35de",
        "deepnote_cell_type": "markdown",
        "id": "maCUNAvZrqC2"
      },
      "source": [
        "Para ser m√°s espec√≠ficos, usted debe cumplir los siguientes objetivos:\n",
        "1. Generar una funci√≥n que permita replicar el gr√°fico expuesto en la imagen (no importa que los colores calcen). [4 puntos]\n",
        "2. Ejecuta la funci√≥n para un `n_samples` igual a 1000, 5000, 10000. [2 puntos]\n",
        "3. Analice y compare el desempe√±o, tiempo de ejecuci√≥n y visualizaciones de cuatro algoritmos de clustering utilizando las 3 configuraciones dadas en `n_samples`. [4 puntos]\n",
        "\n",
        "\n",
        "> ‚ùó Tiene libertad absoluta de escoger los hiper par√°metros de los cluster, sin embargo, se recomienda verificar el dominio de las variables para realizar la segmentaci√≥n.\n",
        "\n",
        "> ‚ùó Recuerde que es obligatorio el uso de plotly.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cell_id": "7f7c25e366754595b13fc2e8116f65a0",
        "deepnote_cell_type": "code",
        "deepnote_to_be_reexecuted": false,
        "execution_millis": 78,
        "execution_start": 1714107108441,
        "id": "i0IZPGPOrqC3",
        "source_hash": null
      },
      "outputs": [],
      "source": [
        "\"\"\"\n",
        "En la siguiente celda se crean los datos ficticios a usar en la secci√≥n 1 del lab.\n",
        "‚ùóNo realice cambios a esta celda a excepci√≥n de n_samples‚ùó\n",
        "\"\"\"\n",
        "\n",
        "# Datos a utilizar\n",
        "\n",
        "# Configuracion\n",
        "n_samples = 5000 #Este par√°metro si lo pueden modificar\n",
        "\n",
        "def create_data(n_samples):\n",
        "\n",
        "    # Lunas\n",
        "    moons = datasets.make_moons(n_samples=n_samples, noise=0.05, random_state=30)\n",
        "    # Blobs\n",
        "    blobs = datasets.make_blobs(n_samples=n_samples, random_state=172)\n",
        "    # Datos desiguales\n",
        "    transformation = [[0.6, -0.6], [-0.4, 0.8]]\n",
        "    mutated = (np.dot(blobs[0], transformation), blobs[1])\n",
        "\n",
        "    # Generamos Dataset\n",
        "    dataset = {\n",
        "        'moons':{\n",
        "            'x': moons[0], 'classes': moons[1], 'n_cluster': 2\n",
        "        },\n",
        "        'blobs':{\n",
        "            'x': blobs[0], 'classes': blobs[1], 'n_cluster': 3\n",
        "        },\n",
        "        'mutated':{\n",
        "            'x': mutated[0], 'classes': mutated[1], 'n_cluster': 3\n",
        "        }\n",
        "    }\n",
        "    return dataset\n",
        "\n",
        "data_sets = create_data(n_samples)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "y51s6f_UtIkc"
      },
      "source": [
        "**Respuestas:**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def plot_scatter(x, y, color):\n",
        "    \"\"\"\n",
        "    Devuelve un ScatterGL para graficar puntos 2D con coloraci√≥n por etiqueta.\n",
        "    x, y: arrays 1D de coordenadas.\n",
        "    color: array 1D de etiquetas (num√©ricas o texto).\n",
        "    \"\"\"\n",
        "    return go.Scattergl(\n",
        "        x=x, y=y, mode=\"markers\",\n",
        "        marker=dict(size=3, color=color, showscale=False),\n",
        "        hoverinfo=\"skip\",\n",
        "        name=\"\",\n",
        "        showlegend=False\n",
        "    )\n",
        "\n",
        "\n",
        "def safe_silhouette(X, labels):\n",
        "    # silhouette necesita >= 2 clusters; si hay ruido (-1) en DBSCAN, lo excluimos.\n",
        "    lbl = np.asarray(labels)\n",
        "    m = lbl != -1\n",
        "    # si todos son ruido o queda un solo cluster tras filtrar, devolvemos NaN\n",
        "    if m.sum() == 0:\n",
        "        return np.nan\n",
        "    u = np.unique(lbl[m])\n",
        "    if u.size < 2:\n",
        "        return np.nan\n",
        "    return float(silhouette_score(X[m], lbl[m], metric=\"euclidean\"))\n",
        "\n",
        "\n",
        "def build_estimators(row_name, k):\n",
        "    # Hiperpar√°metros razonables tras escalar (StandardScaler).\n",
        "    eps_map = {\"moons\": 0.24, \"blobs\": 0.35, \"mutated\": 0.32}\n",
        "    ests = {\n",
        "        \"Kmeans\":  KMeans(n_clusters=k, n_init=10, random_state=42),\n",
        "        \"GMM\":     GaussianMixture(n_components=k, covariance_type=\"full\", random_state=42),\n",
        "        \"WARD\":    AgglomerativeClustering(n_clusters=k, linkage=\"ward\"),\n",
        "        \"DBSCAN\":  DBSCAN(eps=eps_map.get(row_name, 0.3), min_samples=5)\n",
        "    }\n",
        "    # Todos con pipeline de estandarizaci√≥n para homogenizar dominio\n",
        "    pipes = {name: make_pipeline(StandardScaler(), est) for name, est in ests.items()}\n",
        "    return pipes\n",
        "\n",
        "\n",
        "N_COLS = 4  # n√∫mero de columnas del panel\n",
        "\n",
        "def _axis_suffix(row, col, n_cols=N_COLS):\n",
        "    axis_id = (row - 1) * n_cols + col\n",
        "    return \"\" if axis_id == 1 else str(axis_id)\n",
        "\n",
        "def add_panel(fig, X, yhat, r, c, title=None, t_sec=None, sil=None):\n",
        "    tr = plot_scatter(X[:, 0], X[:, 1], yhat)\n",
        "    fig.add_trace(tr, row=r, col=c)\n",
        "\n",
        "    # texto de tiempo y silhouette\n",
        "    if (t_sec is not None) or (sil is not None):\n",
        "        x0 = float(X[:, 0].min())\n",
        "        y0 = float(X[:, 1].max())\n",
        "        ttxt = f\"{t_sec:.2f} [s]\" if t_sec is not None else \"-\"\n",
        "        stxt = f\"{sil:.2f}\" if sil is not None and np.isfinite(sil) else \"NaN\"\n",
        "        txt = f\"{ttxt} | s: {stxt}\"\n",
        "\n",
        "        suf = _axis_suffix(r, c)\n",
        "        fig.add_annotation(\n",
        "            x=x0, y=y0, text=txt, showarrow=False,\n",
        "            xref=f\"x{suf}\", yref=f\"y{suf}\",\n",
        "            font=dict(size=10), align=\"left\"\n",
        "        )\n",
        "\n",
        "    if title:\n",
        "        fig.update_xaxes(title_text=title, row=r, col=c)\n",
        "\n",
        "def benchmark_panel(data_dict, height=800, width=1200):\n",
        "    rows = [\"moons\", \"blobs\", \"mutated\"]\n",
        "    cols = [\"Kmeans\", \"GMM\", \"WARD\", \"DBSCAN\"]\n",
        "    fig = make_subplots(rows=3, cols=4,\n",
        "                        horizontal_spacing=0.04, vertical_spacing=0.08)\n",
        "\n",
        "    for i, rname in enumerate(rows, start=1):\n",
        "        X = data_dict[rname][\"x\"]\n",
        "        k = data_dict[rname][\"n_cluster\"]\n",
        "        models = build_estimators(rname, k)\n",
        "\n",
        "        for j, cname in enumerate(cols, start=1):\n",
        "            pipe = models[cname]\n",
        "            t0 = time.perf_counter()\n",
        "            try:\n",
        "                # KMeans, Agglomerative (WARD) y DBSCAN pasan por aqu√≠\n",
        "                yhat = pipe.fit_predict(X)\n",
        "            except AttributeError:\n",
        "                # GMM no tiene fit_predict: usar fit + predict\n",
        "                pipe.fit(X)\n",
        "                yhat = pipe.predict(X)\n",
        "            t1 = time.perf_counter()\n",
        "\n",
        "            s = safe_silhouette(X, yhat)\n",
        "            add_panel(fig, X, yhat, r=i, c=j, t_sec=t1 - t0, sil=s)\n",
        "\n",
        "\n",
        "    # t√≠tulos de columnas\n",
        "    for j, cname in enumerate(cols, start=1):\n",
        "        fig.add_annotation(text=f\"<b>{cname}</b>\", xref=\"paper\", yref=\"paper\",\n",
        "                           x=(j - 0.5) * (1 / 4), y=1.06, showarrow=False,\n",
        "                           font=dict(size=16))\n",
        "\n",
        "    # t√≠tulos de filas\n",
        "    for i, rname in enumerate(rows, start=1):\n",
        "        fig.add_annotation(text=f\"<b>{rname.upper()}</b>\", xref=\"paper\", yref=\"paper\",\n",
        "                           x=-0.04, y=1 - (i - 0.5) * (1 / 3), showarrow=False,\n",
        "                           font=dict(size=12), textangle=-90)\n",
        "\n",
        "    fig.update_layout(\n",
        "        title=\"Comparaci√≥n de tiempos de ejecuci√≥n por t√©cnica\",\n",
        "        template=\"plotly_white\",\n",
        "        height=height, width=width\n",
        "    )\n",
        "    fig.update_xaxes(showgrid=False, zeroline=False,\n",
        "                 showline=True, linewidth=1, linecolor=\"black\",\n",
        "                 mirror=False, ticks=\"outside\")\n",
        "    fig.update_yaxes(showgrid=False, zeroline=False,\n",
        "                    showline=True, linewidth=1, linecolor=\"black\",\n",
        "                 mirror=False, ticks=\"outside\")\n",
        "    return fig\n",
        "\n",
        "# Ejecutamos para n_samples 1000, 5000, 10000\n",
        "fig_1k  = benchmark_panel(create_data(1000))\n",
        "fig_5k  = benchmark_panel(create_data(5000))\n",
        "fig_10k = benchmark_panel(create_data(10000))\n",
        "\n",
        "# Visualizamos\n",
        "fig_1k.show()\n",
        "fig_5k.show()\n",
        "fig_10k.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Calidad (Silhouette + visual)\n",
        "\n",
        "* Moons (no convexos): KMeans/GMM/Ward cortan las lunas (`s‚âà0.42‚Äì0.47`). DBSCAN respeta la forma pero depende de `eps` (`s‚âà0.33‚Äì0.36`; mejora bajando `eps`/subiendo `min_samples`).\n",
        "* Blobs (esf√©ricos): KMeans, GMM y Ward muy bien (`s‚âà0.78‚Äì0.79`). DBSCAN algo menor (`‚âà0.74‚Äì0.75`) salvo buen ajuste de `eps`.\n",
        "* Mutated (elipses oblicuas): GMM y Ward estables (`s‚âà0.51‚Äì0.55`). DBSCAN puede sobresalir (`‚âà0.74‚Äì0.75`) cuando hay clara separaci√≥n por densidad. KMeans algo por debajo.\n",
        "\n",
        "Tiempos y escalamiento\n",
        "\n",
        "* Ward: el m√°s costoso; crece notablemente con `n` (‚âà0.02s‚Üí0.3s‚Üí1.1‚Äì1.3s).\n",
        "* KMeans / GMM / DBSCAN: r√°pidos y casi lineales para 2D y `n‚â§10k` (d√©cimas de segundo).\n",
        "  KMeans ‚âà DBSCAN ‚â≤ GMM ‚â™ Ward.\n",
        "\n",
        "Elecci√≥n pr√°ctica\n",
        "\n",
        "* Esf√©rico/gaussiano: KMeans (simple, r√°pido) o GMM (covarianzas distintas).\n",
        "* Formas no convexas / outliers: DBSCAN (ajustar `eps`, `min_samples`).\n",
        "* Exploraci√≥n jer√°rquica/√°rbol: Ward (mejor en submuestras).\n",
        "* Siempre: `StandardScaler` (y opcional PCA). Para datos mixtos, usar `ColumnTransformer` en un `Pipeline`."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "cell_id": "13c5cb8067d9415f83b3d497954a437a",
        "deepnote_cell_type": "markdown",
        "id": "3mCbZc86rqC6"
      },
      "source": [
        "## 2. An√°lisis de Satisfacci√≥n de Vuelos. [10 puntos]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "cell_id": "fd6e991646b44f50a4b13f01d1542415",
        "deepnote_cell_type": "markdown",
        "id": "JI33m5jbrqC6"
      },
      "source": [
        "<center>\n",
        "<img src=\"https://media4.giphy.com/media/v1.Y2lkPTZjMDliOTUyb3B5Y3BtbTZwMnB0ZXRyejFpanJkNDl5cGhoeWlsc2k5bGx1MTUwYSZlcD12MV9naWZzX3NlYXJjaCZjdD1n/l4FARHkIFJReGSy2c/giphy.gif\" width=400 />"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "cell_id": "5742dfbd5a2e43778ff250436bab1005",
        "deepnote_cell_type": "markdown",
        "id": "h5k24znirqC7"
      },
      "source": [
        "Habiendo entendido c√≥mo funcionan los modelos de aprendizaje no supervisado, *Don Mathias* le encomienda estudiar la satisfacci√≥n de pasajeros al haber tomado un vuelo en alguna de sus aerolineas. Para esto, el magnate le dispone del dataset `aerolineas_licer.parquet`, el cual contiene el grado de satisfacci√≥n de los clientes frente a diferentes aspectos del vuelo. Las caracter√≠sticas del vuelo se definen a continuaci√≥n:\n",
        "\n",
        "- *Gender*: G√©nero de los pasajeros (Femenino, Masculino)\n",
        "- *Customer Type*: Tipo de cliente (Cliente habitual, cliente no habitual)\n",
        "- *Age*: Edad actual de los pasajeros\n",
        "- *Type of Travel*: Prop√≥sito del vuelo de los pasajeros (Viaje personal, Viaje de negocios)\n",
        "- *Class*: Clase de viaje en el avi√≥n de los pasajeros (Business, Eco, Eco Plus)\n",
        "- *Flight distance*: Distancia del vuelo de este viaje\n",
        "- *Inflight wifi service*: Nivel de satisfacci√≥n del servicio de wifi durante el vuelo (0:No Aplicable; 1-5)\n",
        "- *Departure/Arrival time convenient*: Nivel de satisfacci√≥n con la conveniencia del horario de salida/llegada\n",
        "- *Ease of Online booking*: Nivel de satisfacci√≥n con la facilidad de reserva en l√≠nea\n",
        "- *Gate location*: Nivel de satisfacci√≥n con la ubicaci√≥n de la puerta\n",
        "- *Food and drink*: Nivel de satisfacci√≥n con la comida y la bebida\n",
        "- *Online boarding*: Nivel de satisfacci√≥n con el embarque en l√≠nea\n",
        "- *Seat comfort*: Nivel de satisfacci√≥n con la comodidad del asiento\n",
        "- *Inflight entertainment*: Nivel de satisfacci√≥n con el entretenimiento durante el vuelo\n",
        "- *On-board service*: Nivel de satisfacci√≥n con el servicio a bordo\n",
        "- *Leg room service*: Nivel de satisfacci√≥n con el espacio para las piernas\n",
        "- *Baggage handling*: Nivel de satisfacci√≥n con el manejo del equipaje\n",
        "- *Check-in service*: Nivel de satisfacci√≥n con el servicio de check-in\n",
        "- *Inflight service*: Nivel de satisfacci√≥n con el servicio durante el vuelo\n",
        "- *Cleanliness*: Nivel de satisfacci√≥n con la limpieza\n",
        "- *Departure Delay in Minutes*: Minutos de retraso en la salida\n",
        "- *Arrival Delay in Minutes*: Minutos de retraso en la llegada"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sOoIFHpw5xCW"
      },
      "source": [
        "En consideraci√≥n de lo anterior, realice las siguientes tareas:\n",
        "\n",
        "0. Ingeste el dataset a su ambiente de trabajo.\n",
        "\n",
        "1. Seleccione **s√≥lo las variables num√©ricas del dataset**.  Explique qu√© √©fectos podr√≠a causar el uso de variables categ√≥ricas en un algoritmo no supervisado. [2 punto]\n",
        "\n",
        "2. Realice una visualizaci√≥n de la distribuci√≥n de cada variable y analice cada una de estas distribuciones. [2 punto]\n",
        "\n",
        "3. Bas√°ndose en los gr√°ficos, eval√∫e la necesidad de escalar los datos y explique el motivo de su decisi√≥n. [2 puntos]\n",
        "\n",
        "4. Examine la correlaci√≥n entre las variables mediante un correlograma. [2 puntos]\n",
        "\n",
        "5. De acuerdo con los resultados obtenidos en 4, reduzca la dimensionalidad del conjunto de datos a cuatro variables, justificando su elecci√≥n respecto a las variables que decide eliminar. [2 puntos]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tO6tcVBCtxxS"
      },
      "source": [
        "**Respuesta:**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pzHTZ17xveU_"
      },
      "outputs": [],
      "source": [
        "# PUNTO 0 Carga de datos\n",
        "df = pd.read_parquet(\"aerolineas_lucer.parquet\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# PUNTO 1 Selecci√≥n de variables num√©ricas\n",
        "num_cols = df.select_dtypes(include=\"number\").columns.tolist()\n",
        "X_num = df[num_cols].copy()\n",
        "\n",
        "cat_cols = df.columns.difference(num_cols).tolist()\n",
        "print(f\"N¬∫ columnas num√©ricas: {len(num_cols)}\")\n",
        "print(f\"N¬∫ columnas categ√≥ricas: {len(cat_cols)}\")\n",
        "print(\"Num√©ricas:\", num_cols)\n",
        "print(\"Categ√≥ricas:\", cat_cols)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Respondiendo a la pregunta que se plantea, los efectos de usar variables categ√≥ricas en algoritmos no supervisados podr√≠an ser:\n",
        "* Distancias mal definidas o enga√±osas. K-means, Ward y GMM suponen espacios m√©tricos euclidianos y medias/varianzas; con categ√≥ricas puras esas operaciones no tienen significado. Si se usa label encoding (0,1,2‚Ä¶), se introduce un orden artificial y diferencias num√©ricas inventadas (p. ej., ‚ÄúBusiness‚Äù‚Äì‚ÄúEco‚Äù = 2 no significa nada).\n",
        "* Explosi√≥n dimensional con One-Hot. Convertir categ√≥ricas a one-hot aumenta dimensiones y esparcidad, deteriorando m√©tricas basadas en distancia (peor silhouette, m√°s sensibilidad a escala).\n",
        "* Supuestos de distribuci√≥n. GMM asume gaussianas; no aplica a variables nominales. Ward minimiza varianza intra-grupo; tampoco es coherente con categor√≠as."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# PUNTO 2 Visualizaci√≥n\n",
        "# Clasificaci√≥n autom√°tica de variables\n",
        "likert_cols = [c for c in X_num.columns\n",
        "               if X_num[c].between(0, 5).all() and X_num[c].nunique() <= 6]\n",
        "cont_cols = [c for c in X_num.columns if c not in likert_cols]\n",
        "\n",
        "# Distribuciones de √≠tems tipo Likert (0‚Äì5)\n",
        "m_likert = X_num[likert_cols].melt(var_name=\"Variable\", value_name=\"Valor\")\n",
        "\n",
        "fig_likert = px.histogram(\n",
        "    m_likert, x=\"Valor\", facet_col=\"Variable\", facet_col_wrap=4,\n",
        "    category_orders={\"Valor\": [0, 1, 2, 3, 4, 5]},\n",
        "    histnorm=\"percent\", nbins=6\n",
        ")\n",
        "fig_likert.update_layout(\n",
        "    title=\"Distribuciones: √≠tems de satisfacci√≥n (0‚Äì5)\",\n",
        "    bargap=0.05, showlegend=False, template=\"plotly_white\"\n",
        ")\n",
        "fig_likert.update_xaxes(showgrid=False, zeroline=False, showline=True,\n",
        "                        linewidth=1, linecolor=\"black\", mirror=True, ticks=\"outside\")\n",
        "fig_likert.update_yaxes(title=\"%\", showgrid=True, gridwidth=0.3, gridcolor=\"lightgray\",\n",
        "                        showline=True, linewidth=1, linecolor=\"black\", mirror=True, ticks=\"outside\")\n",
        "\n",
        "fig_likert.show()\n",
        "\n",
        "# Distribuciones de variables continuas (lineal)\n",
        "m_cont = X_num[cont_cols].melt(var_name=\"Variable\", value_name=\"Valor\")\n",
        "\n",
        "fig_cont = px.histogram(\n",
        "    m_cont, x=\"Valor\", facet_col=\"Variable\", facet_col_wrap=2,\n",
        "    nbins=60, histnorm=\"probability density\", marginal=\"box\"\n",
        ")\n",
        "fig_cont.update_layout(\n",
        "    title=\"Distribuciones: variables continuas\",\n",
        "    showlegend=False, template=\"plotly_white\"\n",
        ")\n",
        "fig_cont.update_xaxes(showgrid=False, zeroline=False, showline=True,\n",
        "                      linewidth=1, linecolor=\"black\", mirror=True, ticks=\"outside\")\n",
        "fig_cont.update_yaxes(showgrid=True, gridwidth=0.3, gridcolor=\"lightgray\",\n",
        "                      showline=True, linewidth=1, linecolor=\"black\", mirror=True, ticks=\"outside\")\n",
        "\n",
        "fig_cont.show()\n",
        "\n",
        "# Versiones log1p para colas pesadas (inspecci√≥n)\n",
        "skewed_cols = [c for c in cont_cols if X_num[c].skew() > 1]  # p.ej., distancias y retrasos\n",
        "if len(skewed_cols) > 0:\n",
        "    m_skew = X_num[skewed_cols].apply(np.log1p).melt(\n",
        "        var_name=\"Variable\", value_name=\"Valor_log1p\"\n",
        "    )\n",
        "    fig_cont_log = px.histogram(\n",
        "        m_skew, x=\"Valor_log1p\", facet_col=\"Variable\", facet_col_wrap=2,\n",
        "        nbins=60, histnorm=\"probability density\", marginal=\"box\"\n",
        "    )\n",
        "    fig_cont_log.update_layout(\n",
        "        title=\"Distribuciones (log1p): variables con cola pesada\",\n",
        "        showlegend=False, template=\"plotly_white\"\n",
        "    )\n",
        "    fig_cont_log.update_xaxes(showgrid=False, zeroline=False, showline=True,\n",
        "                              linewidth=1, linecolor=\"black\", mirror=True, ticks=\"outside\")\n",
        "    fig_cont_log.update_yaxes(showgrid=True, gridwidth=0.3, gridcolor=\"lightgray\",\n",
        "                              showline=True, linewidth=1, linecolor=\"black\", mirror=True, ticks=\"outside\")\n",
        "    fig_cont_log.show()\n",
        "\n",
        "# Resumen num√©rico para apoyar el an√°lisis del Punto 3\n",
        "summary = X_num[cont_cols].agg(\n",
        "    [\"min\", \"median\", \"mean\", \"std\", lambda s: s.skew(), \"max\"]\n",
        ").T.rename(columns={\"<lambda_0>\": \"skew\"})\n",
        "display(summary)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Se debe escalar, puesto que los algoritmos tipo KMeans, GMM, Ward, DBSCAN trabajan con distancias. Sin escalar, variables como Flight Distance y Delays dominar√≠an la m√©trica, anulando el aporte de los √≠tems 0‚Äì5. Adem√°s, para variables con colas largas conviene transformar (p. ej. `log1p`) antes de estandarizar o, en su defecto, usar un RobustScaler.\n",
        "\n",
        "Con lo que el criterio adoptado es el siguiente:\n",
        "\n",
        "* Skewed: `Flight Distance`, `Departure Delay in Minutes`, `Arrival Delay in Minutes` ‚Üí `log1p` + `RobustScaler`.\n",
        "* Casi sim√©tricas/ordinales (0‚Äì5) + `Age` ‚Üí `StandardScaler`.\n",
        "* `id`: eliminar (identificador).\n",
        "\n",
        "De esta manera todas las variables quedan comparables (media‚âà0, escala‚âà1) y las colas extremas de retrasos/distancia quedan contenidas."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# PUNTO 3 Escalado\n",
        "X_num = X_num.drop(columns=[\"id\"], errors=\"ignore\")\n",
        "\n",
        "# Identificaci√≥n de columnas\n",
        "likert_cols = [c for c in X_num.columns\n",
        "               if X_num[c].between(0, 5).all() and X_num[c].nunique() <= 6]\n",
        "cont_cols   = [c for c in X_num.columns if c not in likert_cols]\n",
        "skewed_cols = [c for c in cont_cols if X_num[c].skew() > 1]\n",
        "symm_cols   = [c for c in cont_cols if c not in skewed_cols]\n",
        "\n",
        "# Escalado\n",
        "log1p = FunctionTransformer(np.log1p, validate=False)\n",
        "pipe_skew = make_pipeline(log1p, RobustScaler())\n",
        "ct = ColumnTransformer(\n",
        "    transformers=[\n",
        "        (\"skew\", pipe_skew, skewed_cols),\n",
        "        (\"std\",  StandardScaler(), symm_cols + likert_cols),\n",
        "    ],\n",
        "    remainder=\"drop\"\n",
        ")\n",
        "\n",
        "Z = ct.fit_transform(X_num)\n",
        "scaled_cols = skewed_cols + symm_cols + likert_cols\n",
        "X_scaled = pd.DataFrame(Z, columns=scaled_cols, index=X_num.index)\n",
        "\n",
        "# Verificaci√≥n num√©rica\n",
        "check = X_scaled.agg([\"mean\", \"std\"]).T.round(3)\n",
        "display(check)\n",
        "\n",
        "# Utilidades de ploteo (proporciones, t√≠tulos y rangos legibles)\n",
        "def _wrap(txt, width=22):\n",
        "    parts, line, L = [], [], 0\n",
        "    for w in str(txt).split():\n",
        "        L_new = L + len(w) + (1 if L>0 else 0)\n",
        "        if L_new > width:\n",
        "            parts.append(\" \".join(line)); line=[w]; L=len(w)\n",
        "        else:\n",
        "            line.append(w); L=L_new\n",
        "    parts.append(\" \".join(line))\n",
        "    return \"<br>\".join(parts)\n",
        "\n",
        "def _hist_facetas(df_long, x, facet_col, facet_wrap=4, nbins=50,\n",
        "                  title=\"\", width=1200, height_per_row=260, font_size=13,\n",
        "                  xquant_range=None, show_legend=False, barmode=None, opacity=None,\n",
        "                  marginal=\"box\"):\n",
        "    \"\"\"Crea un histograma. ‚Å†‚ÄØxquant_range‚ÄØ‚Å†=(q_low, q_high) aplica el mismo rango X.\"\"\"\n",
        "    # n¬∫ filas estimadas para el alto\n",
        "    n_vars = df_long[facet_col].nunique()\n",
        "    rows   = int(np.ceil(n_vars / facet_wrap))\n",
        "    fig = px.histogram(\n",
        "        df_long, x=x, facet_col=facet_col, facet_col_wrap=facet_wrap,\n",
        "        nbins=nbins, histnorm=\"probability density\",\n",
        "        barmode=barmode, opacity=opacity, marginal=marginal,\n",
        "        color=df_long.columns.difference([x, facet_col]).tolist()[0] if show_legend else None\n",
        "    )\n",
        "    # para limpiar y envolver t√≠tulos\n",
        "    for a in fig.layout.annotations:\n",
        "        if a.text and f\"{facet_col}=\" in a.text:\n",
        "            a.text = _wrap(a.text.split(\"=\",1)[1])\n",
        "    # rango com√∫n en eje-x si se solicita\n",
        "    if xquant_range is not None:\n",
        "        fig.update_xaxes(range=list(xquant_range))\n",
        "\n",
        "    fig.update_layout(\n",
        "        title=title, template=\"plotly_white\", showlegend=show_legend,\n",
        "        legend_title_text=\"\", font=dict(size=font_size),\n",
        "        width=width, height=120 + height_per_row*rows\n",
        "    )\n",
        "    fig.update_xaxes(showgrid=False, zeroline=False, showline=True,\n",
        "                     linewidth=1, linecolor=\"black\", mirror=True, ticks=\"outside\",\n",
        "                     title=\"Valor\")\n",
        "    fig.update_yaxes(showgrid=True, gridwidth=0.3, gridcolor=\"lightgray\",\n",
        "                     showline=True, linewidth=1, linecolor=\"black\", mirror=True, ticks=\"outside\",\n",
        "                     title=\"densidad\")\n",
        "    return fig\n",
        "\n",
        "# Comparativo ANTES vs DESPU√âS para variables sesgadas\n",
        "# Para comparar en la misma escala, al \"antes\" se le aplica log1p.\n",
        "if len(skewed_cols) > 0:\n",
        "    before_log = X_num[skewed_cols].apply(np.log1p)\n",
        "    m_before = before_log.melt(var_name=\"Variable\", value_name=\"Valor\")\n",
        "    m_before[\"Estado\"] = \"Antes (log1p)\"\n",
        "    m_after  = X_scaled[skewed_cols].melt(var_name=\"Variable\", value_name=\"Valor\")\n",
        "    m_after[\"Estado\"] = \"Despu√©s (log1p + Robust)\"\n",
        "    m_comp = pd.concat([m_before, m_after], ignore_index=True)\n",
        "\n",
        "    # Rango x por cuantiles combinados para que las barras sean anchas y comparables\n",
        "    q_low  = m_comp[\"Valor\"].quantile(0.005)\n",
        "    q_high = m_comp[\"Valor\"].quantile(0.995)\n",
        "\n",
        "    fig_comp = _hist_facetas(\n",
        "        m_comp, x=\"Valor\", facet_col=\"Variable\", facet_wrap=3, nbins=40,\n",
        "        title=\"Antes vs. Despu√©s (variables con cola pesada)\",\n",
        "        width=1200, height_per_row=260, font_size=13,\n",
        "        xquant_range=(q_low, q_high),\n",
        "        show_legend=True, barmode=\"overlay\", opacity=0.55, marginal=None\n",
        "    )\n",
        "    fig_comp.show()\n",
        "\n",
        "# Todas las variables DESPU√âS de escalar (lo separamos en lotes para que sea m√°s legible)\n",
        "all_cols = X_scaled.columns.tolist()\n",
        "batch_size = 8\n",
        "n_batches  = int(np.ceil(len(all_cols)/batch_size))\n",
        "\n",
        "for k in range(n_batches):\n",
        "    cols_k = all_cols[k*batch_size : (k+1)*batch_size]\n",
        "    m_scaled_k = X_scaled[cols_k].melt(var_name=\"Variable\", value_name=\"Valor\")\n",
        "    # Rango x por cuantiles para evitar colas que aplasten las barras\n",
        "    q_low  = m_scaled_k[\"Valor\"].quantile(0.005)\n",
        "    q_high = m_scaled_k[\"Valor\"].quantile(0.995)\n",
        "    fig_scaled_k = _hist_facetas(\n",
        "        m_scaled_k, x=\"Valor\", facet_col=\"Variable\", facet_wrap=4, nbins=40,\n",
        "        title=f\"Distribuciones despu√©s de escalar ‚Äî Lote {k+1} de {n_batches}\",\n",
        "        width=1200, height_per_row=260, font_size=13,\n",
        "        xquant_range=(q_low, q_high),\n",
        "        show_legend=False, barmode=None, opacity=None, marginal=\"box\"\n",
        "    )\n",
        "    fig_scaled_k.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# PUNTO 4 Correlograma (matriz de correlaciones)\n",
        "\n",
        "# Usamos correlaci√≥n de Spearman (robusta a no-linealidades mon√≥tonas\n",
        "# y a la naturaleza ordinal de los √≠tems 0‚Äì5). Visualizamos la mitad\n",
        "# inferior de la matriz para facilitar lectura y ordenamos las variables\n",
        "# por \"conectividad\" (suma de correlaciones absolutas) para agrupar afinidades.\n",
        "\n",
        "# Matriz de correlaciones (Spearman)\n",
        "C = X_scaled.corr(method=\"spearman\")\n",
        "\n",
        "# Orden de columnas por conectividad (aglomera variables afines sin jerarqu√≠a)\n",
        "order = (np.abs(C).sum().sort_values(ascending=False)).index\n",
        "C = C.loc[order, order]\n",
        "\n",
        "# Enmascaramos tri√°ngulo superior para no duplicar informaci√≥n\n",
        "mask = np.triu(np.ones_like(C, dtype=bool), k=1)\n",
        "C_masked = C.mask(mask)\n",
        "\n",
        "# Correlograma con Plotly\n",
        "fig_corr = px.imshow(\n",
        "    C_masked,\n",
        "    zmin=-1, zmax=1,\n",
        "    color_continuous_scale=\"RdBu\",\n",
        "    aspect=\"auto\",\n",
        "    labels=dict(color=\"œÅ (Spearman)\")\n",
        ")\n",
        "fig_corr.update_layout(\n",
        "    title=\"Correlograma (Spearman) ‚Äî mitad inferior\",\n",
        "    template=\"plotly_white\",\n",
        "    width=1000, height=900,\n",
        "    coloraxis_colorbar=dict(title=\"œÅ\")\n",
        ")\n",
        "fig_corr.update_xaxes(side=\"bottom\", showgrid=False, tickangle=45)\n",
        "fig_corr.update_yaxes(showgrid=False)\n",
        "fig_corr.show()\n",
        "\n",
        "# Tabla de pares m√°s correlacionados\n",
        "pairs = (\n",
        "    C.where(~np.eye(len(C), dtype=bool))\n",
        "     .stack()\n",
        "     .reset_index()\n",
        "     .rename(columns={\"level_0\":\"var1\", \"level_1\":\"var2\", 0:\"rho\"})\n",
        ")\n",
        "\n",
        "top_pos = pairs.sort_values(\"rho\", ascending=False).head(10).reset_index(drop=True)\n",
        "top_neg = pairs.sort_values(\"rho\", ascending=True).head(10).reset_index(drop=True)\n",
        "\n",
        "print(\"Top 10 correlaciones POSITIVAS:\")\n",
        "display(top_pos)\n",
        "print(\"Top 10 correlaciones NEGATIVAS:\")\n",
        "display(top_neg)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "De los resultados del correlograma, se pueden extraer las siguientes conclusiones:\n",
        "\n",
        "* El mapa muestra tres grupos principales:\n",
        "  - (G1) Experiencia a bordo: {Cleanliness, Seat comfort, Inflight entertainment, Food and drink, On-board service, Leg room service}\n",
        "  - (G2) Experiencia digital/embarque: {Inflight wifi service, Ease of Online booking, Online boarding}\n",
        "  - (G3) Operaci√≥n/puntualidad: {Departure Delay in Minutes, Arrival Delay in Minutes}\n",
        "\n",
        "Y dos variables casi independientes: {Flight Distance, Age}.\n",
        "\n",
        "* Redundancias fuertes dentro de G1, G2 y G3 (œÅ altos): para seleccionar 4 variables en el punto 5 podr√≠amos:\n",
        "\n",
        "  * tomar 1 representante por bloque (p.ej., `Cleanliness` para G1, `Inflight wifi service` o `Ease of Online booking` para G2, y uno solo entre `Departure Delay`/`Arrival Delay` para G3), y\n",
        "  * a√±adir una dimensi√≥n independiente como `Flight Distance` (o `Age`) para capturar variaci√≥n no explicada por satisfacci√≥n ni puntualidad.\n",
        "\n",
        "* Las negativas m√°s grandes son peque√±as (‚âà‚àí0.05): el dataset no refleja relaciones inversas fuertes (al menos de forma mon√≥tona). Esto respalda que la reducci√≥n debe basarse m√°s en agrupamientos por redundancia positiva que por conflictos.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# PUNTO 5 Selecci√≥n de variables\n",
        "\n",
        "# Matriz de correlaci√≥n (Spearman) y centralidad (suma de |œÅ|)\n",
        "C = X_scaled.corr(method=\"spearman\")\n",
        "centralidad = np.abs(C).sum()\n",
        "\n",
        "# Definimos grupos seg√∫n el correlograma del punto 4\n",
        "G1 = [\"Cleanliness\", \"Seat comfort\", \"Inflight entertainment\",\n",
        "      \"Food and drink\", \"On-board service\", \"Leg room service\"]\n",
        "G2 = [\"Inflight wifi service\", \"Ease of Online booking\", \"Online boarding\"]\n",
        "G3 = [\"Departure Delay in Minutes\", \"Arrival Delay in Minutes\"]\n",
        "INDEP = [\"Flight Distance\", \"Age\"]\n",
        "\n",
        "# Elegimos representantes por m√°xima centralidad en cada grupo\n",
        "def pick_best(cols):\n",
        "    cols = [c for c in cols if c in X_scaled.columns]\n",
        "    return centralidad.loc[cols].sort_values(ascending=False).index[0]\n",
        "\n",
        "rep_G1 = pick_best(G1)\n",
        "rep_G2 = pick_best(G2)\n",
        "rep_G3 = pick_best(G3)\n",
        "\n",
        "# Entre las casi independientes, preferimos la m√°s informativa.\n",
        "# Toamos la de de mayor varianza o centralidad:\n",
        "rep_INDEP = centralidad.loc[[c for c in INDEP if c in X_scaled.columns]].sort_values(ascending=False).index[0]\n",
        "# Para este dataset esto seleccionar√≠a \"Flight Distance\".\n",
        "\n",
        "seleccion = [rep_G1, rep_G2, rep_G3, rep_INDEP]\n",
        "\n",
        "# Dataset reducido a 4 variables\n",
        "X_reduced = X_scaled[seleccion].copy()\n",
        "\n",
        "# Hacemos un reporte\n",
        "desechadas = [c for c in X_scaled.columns if c not in seleccion]\n",
        "print(\"Variables seleccionadas (4):\", seleccion)\n",
        "print(\"Variables descartadas (por redundancia o baja conectividad):\", desechadas)\n",
        "\n",
        "# Correlograma de las 4 elegidas para verificar ortogonalidad relativa\n",
        "fig_small = px.imshow(\n",
        "    X_reduced.corr(method=\"spearman\"), zmin=-1, zmax=1,\n",
        "    color_continuous_scale=\"RdBu\", aspect=\"auto\",\n",
        "    labels=dict(color=\"œÅ (Spearman)\")\n",
        ")\n",
        "fig_small.update_layout(title=\"Correlaciones entre las 4 variables seleccionadas\",\n",
        "                        template=\"plotly_white\", width=500, height=450)\n",
        "fig_small.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "cell_id": "4b6c047d994f40ea9e78a36a777042e0",
        "deepnote_cell_type": "markdown",
        "id": "PNGfTgtkrqC9"
      },
      "source": [
        "## 3. Preprocesamiento üé≠. [10 puntos]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "cell_id": "713b3f0e61dd4841bb5b38c730d344d5",
        "deepnote_cell_type": "markdown",
        "id": "6RZD0fMNrqC-"
      },
      "source": [
        "<center>\n",
        "<img src=\"https://media.tenor.com/R_WseIIwQ8QAAAAM/beavis-computer.gif\" width=400>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "cell_id": "98400c7b5fec4af193eec3601f53891e",
        "deepnote_cell_type": "markdown",
        "id": "J6d4VEOTrqC-"
      },
      "source": [
        "Tras quedar satisfecho con los resultados presentados en el punto 2, el due√±o de la empresa ha solicitado que se preprocesen los datos mediante un `pipeline`. Es crucial que este proceso tenga en cuenta las observaciones derivadas de los an√°lisis anteriores. Adicionalmente, ha expresado su inter√©s en visualizar el conjunto de datos en un gr√°fico de dos o tres dimensiones.\n",
        "\n",
        "Bas√°ndose en los an√°lisis realizados anteriormente:\n",
        "1. Cree un `pipeline` que incluya PCA, utilizando las consideraciones mencionadas previamente para proyectar los datos a dos dimensiones. [4 puntos]\n",
        "2. Grafique los resultados obtenidos y comente lo visualizado. [6 puntos]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "paDSaGoq0OUp"
      },
      "source": [
        "**Respuestas:**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cell_id": "ad1e70818ad748638ca0927b07a76125",
        "deepnote_cell_type": "code",
        "id": "gBYG238wrqC-"
      },
      "outputs": [],
      "source": [
        "# PUNTO 1 Pipeline con PCA a 2D\n",
        "def make_pca_pipeline(selected_cols=None, whiten=False, random_state=42):\n",
        "    \"\"\"\n",
        "    Devuelve un Pipeline: (ColumnTransformer -> StandardScaler -> PCA(2D)).\n",
        "    - selected_cols: lista de columnas a incluir (si None, usa todas las num√©ricas X_num.columns).\n",
        "    - whiten: True para blanquear componentes (opcional).\n",
        "    \"\"\"\n",
        "    # Selecci√≥n de columnas a usar (por defecto, todas las num√©ricas ya depuradas)\n",
        "    cols = list(selected_cols) if selected_cols is not None else list(X_num.columns)\n",
        "\n",
        "    # Particionamos seg√∫n la regla de la Secci√≥n 2\n",
        "    skew_cols = [c for c in skewed_cols if c in cols]\n",
        "    std_cols  = [c for c in (symm_cols + likert_cols) if c in cols]\n",
        "\n",
        "    # Preprocesamiento por columna (log1p+Robust para colas; Standard para el resto)\n",
        "    ct = ColumnTransformer(\n",
        "        transformers=[\n",
        "            (\"skew\", make_pipeline(FunctionTransformer(np.log1p, validate=False), RobustScaler()), skew_cols),\n",
        "            (\"std\",  StandardScaler(), std_cols),\n",
        "        ],\n",
        "        remainder=\"drop\",\n",
        "        verbose_feature_names_out=False\n",
        "    )\n",
        "\n",
        "    # Pipeline completo: preproc -> estandarizaci√≥n global -> PCA(2)\n",
        "    pipe = Pipeline(steps=[\n",
        "        (\"pre\", ct),\n",
        "        # Igualar varianzas de ambos bloques antes de PCA (evita que uno domine)\n",
        "        (\"post_std\", StandardScaler(with_mean=True, with_std=True)),\n",
        "        (\"pca\", PCA(n_components=2, whiten=whiten, random_state=random_state))\n",
        "    ])\n",
        "    return pipe\n",
        "\n",
        "# Construcci√≥n del pipeline\n",
        "pca_pipe = make_pca_pipeline(selected_cols=seleccion, whiten=False, random_state=42)\n",
        "\n",
        "# Proyecci√≥n a 2D\n",
        "# Entrada esperada: DataFrame num√©rico sin 'id' (X_num).\n",
        "X_pca2 = pca_pipe.fit_transform(X_num)         # ndarray (n_samples, 2)\n",
        "X_pca2_df = pd.DataFrame(X_pca2, columns=[\"PC1\", \"PC2\"], index=X_num.index)\n",
        "\n",
        "# Vemos 5 filas para verificar\n",
        "display(X_pca2_df.head())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# PCA 2D ‚Äî visualizaci√≥n\n",
        "\n",
        "# Construimos un DataFrame de ploteo con metadata √∫til para colorear\n",
        "pca_plot = X_pca2_df.copy()\n",
        "\n",
        "# Colorear por Arrival Delay (sin escalar, para interpretar minutos); usar log1p para el color\n",
        "if \"Arrival Delay in Minutes\" in X_num.columns:\n",
        "    pca_plot[\"ArrivalDelay_min\"] = X_num[\"Arrival Delay in Minutes\"].clip(lower=0)  # aseguramos no-negativo\n",
        "    pca_plot[\"ArrivalDelay_log1p\"] = np.log1p(pca_plot[\"ArrivalDelay_min\"])\n",
        "else:\n",
        "    pca_plot[\"ArrivalDelay_log1p\"] = 0.0  # fallback\n",
        "\n",
        "# Simbolizamos por terciles de distancia de vuelo (corto/medio/largo)\n",
        "if \"Flight Distance\" in X_num.columns:\n",
        "    pca_plot[\"FlightDist_cat\"] = pd.qcut(X_num[\"Flight Distance\"], q=3, labels=[\"Corta\",\"Media\",\"Larga\"])\n",
        "else:\n",
        "    pca_plot[\"FlightDist_cat\"] = \"NA\"\n",
        "\n",
        "# Varianza explicada (para el t√≠tulo)\n",
        "vr = pca_pipe.named_steps[\"pca\"].explained_variance_ratio_\n",
        "ttl = f\"PCA 2D ‚Äî Var. explicada: PC1={vr[0]:.1%}, PC2={vr[1]:.1%}\"\n",
        "\n",
        "# Scatter 2D con Plotly: color continuo (retraso) + s√≠mbolo (distancia)\n",
        "fig_pca2 = px.scatter(\n",
        "    pca_plot, x=\"PC1\", y=\"PC2\",\n",
        "    color=\"ArrivalDelay_log1p\", color_continuous_scale=\"Viridis\",\n",
        "    symbol=\"FlightDist_cat\", opacity=0.6,\n",
        "    title=ttl, labels={\"ArrivalDelay_log1p\": \"log1p(Delay [min])\"}\n",
        ")\n",
        "fig_pca2.update_layout(template=\"plotly_white\", width=900, height=650, legend_title_text=\"Distancia\")\n",
        "fig_pca2.update_xaxes(showgrid=False, showline=True, linecolor=\"black\", mirror=True, ticks=\"outside\")\n",
        "fig_pca2.update_yaxes(showgrid=False, showline=True, linecolor=\"black\", mirror=True, ticks=\"outside\")\n",
        "fig_pca2.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Notamos que las dos primeras componentes explican ‚âà60.3% de la varianza (PC1=35.2%, PC2=25.1%), por lo que la proyecci√≥n conserva una fracci√≥n relevante de la estructura. Veamos lo que revela la figura:\n",
        "\n",
        "* Gradiente de color (log1p(Delay)): se intensifica claramente hacia PC2 alto. Indica que los retrasos est√°n fuertemente alineados con la segunda componente; coherente con el bloque ‚Äúpuntualidad‚Äù aislado en el correlograma.\n",
        "* S√≠mbolos por distancia (Corta/Media/Larga): se observa estratificaci√≥n suave, especialmente hacia PC2 alto y zonas de PC1 intermedio; la distancia aporta variaci√≥n adicional, pero menos dominante que los delays (consistente con sus bajas correlaciones con el resto).\n",
        "* Bandas diagonales en la zona inferior: son esperables al aplicar PCA sobre muchas variables tipo Likert (1‚Äì5); la discretizaci√≥n origina ‚Äúniveles‚Äù que, combinados linealmente, producen bandas en el plano.\n",
        "* PC1 parece capturar la mezcla ‚Äúcalidad a bordo + experiencia digital‚Äù (bloques altamente correlacionados en el correlograma). Zonas de PC1 m√°s altas tienden a asociarse con mejores valoraciones de limpieza/confort/entretenimiento/online.\n",
        "* La proyecci√≥n 2D separa (de forma continua) observaciones con mayor retraso (parte superior) y mejor experiencia (hacia la derecha en PC1)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "cell_id": "bd281470d3054764a63d857cfa7d52a6",
        "deepnote_cell_type": "text-cell-h2",
        "formattedRanges": [],
        "id": "7ENoOtIIrqC_"
      },
      "source": [
        "## 4. Outliers üö´üôÖ‚Äç‚ôÄÔ∏è‚ùåüôÖ‚Äç‚ôÇÔ∏è [10 puntos]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "cell_id": "db89e9c9f35c44abbd8991180226c0ea",
        "deepnote_cell_type": "markdown",
        "id": "fbGw6Sa-rqC_"
      },
      "source": [
        "<center>\n",
        "<img src=\"https://joachim-gassen.github.io/images/ani_sim_bad_leverage.gif\" width=250>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "cell_id": "3e2f59fa12954641af7a854a4e203694",
        "deepnote_cell_type": "markdown",
        "id": "nl_ccu9brqDA"
      },
      "source": [
        "Con el objetivo de mantener la claridad en su an√°lisis, Don Mathias le ha solicitado entrenar un modelo que identifique pasajeros con comportamientos altamente at√≠picos.\n",
        "\n",
        "1. Utilice `IsolationForest` para clasificar las anomal√≠as del dataset (sin aplicar PCA), configurando el modelo para que s√≥lo el 1% de los datos sean considerados an√≥malos. Aseg√∫rese de integrar esta tarea dentro de un `pipeline`. [3 puntos]\n",
        "\n",
        "2. Visualice los resultados en el gr√°fico de dos dimensiones previamente creado. [3 puntos]\n",
        "\n",
        "3. ¬øC√≥mo evaluar√≠a el rendimiento de su modelo en la detecci√≥n de anomal√≠as? [4 puntos]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b5cS1FR00NlF"
      },
      "source": [
        "**Respuestas:**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cell_id": "be86896911244aa89e3b5f3f00a286af",
        "deepnote_cell_type": "code",
        "id": "iaPZFmjyrqDA"
      },
      "outputs": [],
      "source": [
        "# PUNTO 1\n",
        "# Pipeline: preprocesamiento (Secci√≥n 2) + estandarizaci√≥n global + IsolationForest\n",
        "log1p = FunctionTransformer(np.log1p, validate=False)\n",
        "\n",
        "ct = ColumnTransformer(\n",
        "    transformers=[\n",
        "        (\"skew\", make_pipeline(log1p, RobustScaler()),\n",
        "         [c for c in skewed_cols if c in X_num.columns]),\n",
        "        (\"std\", StandardScaler(),\n",
        "         [c for c in (symm_cols + likert_cols) if c in X_num.columns]),\n",
        "    ],\n",
        "    remainder=\"drop\",\n",
        "    verbose_feature_names_out=False\n",
        ")\n",
        "\n",
        "ifo = IsolationForest(\n",
        "    contamination=0.01,       # 1% de anomal√≠as objetivo seg√∫n enunciado\n",
        "    n_estimators=300,\n",
        "    max_samples=\"auto\",\n",
        "    bootstrap=False,\n",
        "    n_jobs=-1,\n",
        "    random_state=42\n",
        ")\n",
        "\n",
        "iso_pipe = Pipeline(steps=[\n",
        "    (\"pre\", ct),\n",
        "    (\"post_std\", StandardScaler(with_mean=True, with_std=True)),\n",
        "    (\"clf\", ifo)\n",
        "])\n",
        "\n",
        "# Entrenamiento y etiquetas de anomal√≠a\n",
        "iso_pipe.fit(X_num)\n",
        "\n",
        "# Etiquetas: 1 = normal, -1 = anomal√≠a\n",
        "y_pred_iso = iso_pipe.named_steps[\"clf\"].predict(\n",
        "    iso_pipe.named_steps[\"post_std\"].transform(iso_pipe.named_steps[\"pre\"].transform(X_num))\n",
        ")\n",
        "\n",
        "# Score (mayor = m√°s normal; menor = m√°s an√≥malo)\n",
        "scores = iso_pipe.named_steps[\"clf\"].decision_function(\n",
        "    iso_pipe.named_steps[\"post_std\"].transform(iso_pipe.named_steps[\"pre\"].transform(X_num))\n",
        ")\n",
        "\n",
        "# Anexamos resultados al √≠ndice de X_num para facilitar el punto 2 (gr√°fico sobre PCA 2D ya calculado)\n",
        "outlier_df = pd.DataFrame({\n",
        "    \"anomaly_label\": (y_pred_iso == -1).astype(int),  # 1=an√≥malo, 0=normal\n",
        "    \"iso_score\": scores\n",
        "}, index=X_num.index)\n",
        "\n",
        "print(\"Fracci√≥n marcada como anomal√≠a:\", outlier_df[\"anomaly_label\"].mean().round(4))\n",
        "display(outlier_df.head())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Unimos PCA 2D con etiquetas de anomal√≠a\n",
        "pca_out = X_pca2_df.join(outlier_df[[\"anomaly_label\", \"iso_score\"]])\n",
        "\n",
        "# Particiones vectorizadas\n",
        "mask_anom = pca_out[\"anomaly_label\"].eq(1)\n",
        "norm = pca_out.loc[~mask_anom]\n",
        "anom = pca_out.loc[mask_anom]\n",
        "\n",
        "# Scatter Plotly: normales al fondo; an√≥malos encima\n",
        "fig = go.Figure()\n",
        "\n",
        "# Normales (gris transl√∫cido, muchos puntos ‚Üí Scattergl)\n",
        "fig.add_trace(go.Scattergl(\n",
        "    x=norm[\"PC1\"], y=norm[\"PC2\"], mode=\"markers\", name=\"Normal\",\n",
        "    marker=dict(size=3, color=\"rgba(120,120,120,0.25)\"),\n",
        "    hoverinfo=\"skip\"\n",
        "))\n",
        "\n",
        "# An√≥malos en color rojo\n",
        "fig.add_trace(go.Scattergl(\n",
        "    x=anom[\"PC1\"], y=anom[\"PC2\"], mode=\"markers\",\n",
        "    name=f\"An√≥malos ({mask_anom.mean():.1%})\",\n",
        "    marker=dict(size=8, color=\"#d62728\", line=dict(color=\"white\", width=0.6)),\n",
        "    text=np.round(anom[\"iso_score\"], 3),\n",
        "    hovertemplate=\"PC1=%{x:.2f}<br>PC2=%{y:.2f}<br>iso_score=%{text}<extra></extra>\"\n",
        "))\n",
        "\n",
        "fig.update_layout(\n",
        "    title=\"IsolationForest (1%) sobre PCA 2D\",\n",
        "    template=\"plotly_white\", width=900, height=650,\n",
        "    legend_title_text=\"\"\n",
        ")\n",
        "fig.update_xaxes(showgrid=False, showline=True, linecolor=\"black\", mirror=True, ticks=\"outside\", title=\"PC1\")\n",
        "fig.update_yaxes(showgrid=False, showline=True, linecolor=\"black\", mirror=True, ticks=\"outside\", title=\"PC2\")\n",
        "fig.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "La forma en que evaluar√≠a el rendimiento del modelo de IsolationForest para detecci√≥n de anomal√≠as, considerando que no tenemos etiquetas reales de outliers, es mediante una combinaci√≥n de chequeos internos, m√©tricas proxy y an√°lisis de robustez:\n",
        "\n",
        "1. Cheques internos\n",
        "\n",
        "* Verificar contaminaci√≥n lograda (\\~1%).\n",
        "* Revisar la distribuci√≥n del `iso_score` (cola inferior clara) y la ubicaci√≥n de outliers en zonas de baja densidad del PCA (sanity check).\n",
        "\n",
        "2. M√©trica con ‚Äúground truth‚Äù proxy\n",
        "\n",
        "* Definir etiquetas proxy por extremos operativos (p. ej., `Arrival/Departure Delay` > p99 o `Flight Distance` > p99).\n",
        "* Medir Precision, Recall, F1 de `anomaly_label` vs. proxy.\n",
        "* Usar el ranking `-decision_function` para calcular ROC-AUC y PR-AUC (m√°s alto = mejor discriminaci√≥n).\n",
        "\n",
        "3. Robustez\n",
        "* Estabilidad por remuestreo: re-entrenar en submuestras y comparar el top 1% con Jaccard medio (alto ‚áí conjunto de outliers consistente).\n",
        "\n",
        "Con buenos valores en (2) y estabilidad en (3), m√°s la coherencia visual de (1), el IsolationForest puede considerarse bien calibrado para este escenario.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "cell_id": "3871e2fe5bdd422dbdbfaebf75503ae3",
        "deepnote_cell_type": "markdown",
        "id": "zQFTklmVrqDB"
      },
      "source": [
        "## 5. M√©tricas de Desempe√±o üöÄ [10 puntos]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "cell_id": "236333de6dd445c182aefcc507589325",
        "deepnote_cell_type": "markdown",
        "id": "YpNj4wbPrqDB"
      },
      "source": [
        "<center>\n",
        "<img src=\"https://i.imgflip.com/6xz0ij.gif\" width=400>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "cell_id": "a7e1ceb91be94b1da2ab8be97dfac999",
        "deepnote_cell_type": "markdown",
        "id": "CR3hzRxrrqDB"
      },
      "source": [
        "Motivado por incrementar su fortuna, Don Mathias le solicita entrenar un modelo que le permita segmentar a los pasajeros en grupos distintos, con el objetivo de optimizar las diversas campa√±as de marketing dise√±adas por su equipo. Para ello, le se pide realizar las siguientes tareas:\n",
        "\n",
        "1. Utilizar el modelo **Gaussian Mixture** y explore diferentes configuraciones de n√∫mero de cl√∫sters, espec√≠ficamente entre 3 y 8. Aseg√∫rese de integrar esta operaci√≥n dentro de un `pipeline`. [4 puntos]\n",
        "2. Explique cu√°l ser√≠a el criterio adecuado para seleccionar el n√∫mero √≥ptimo de cl√∫sters. **Justifique de forma estadistica y a traves de gr√°ficos.** [6 puntos]\n",
        "\n",
        "> **HINT:** Se recomienda investigar sobre los criterios AIC y BIC para esta tarea."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Jt_T_zTg0MXB"
      },
      "source": [
        "**Respuestas:**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cell_id": "6d3d1bb3fda14321984466d9101a775a",
        "deepnote_cell_type": "code",
        "id": "5GeUb9J3rqDB"
      },
      "outputs": [],
      "source": [
        "# Pipeline constructor (preprocesamiento + GMM)\n",
        "def make_gmm_pipeline(n_components, covariance_type=\"full\", random_state=42):\n",
        "    log1p = FunctionTransformer(np.log1p, validate=False)\n",
        "    ct = ColumnTransformer(\n",
        "        transformers=[\n",
        "            (\"skew\", make_pipeline(log1p, RobustScaler()),\n",
        "             [c for c in skewed_cols if c in X_num.columns]),\n",
        "            (\"std\",  StandardScaler(),\n",
        "             [c for c in (symm_cols + likert_cols) if c in X_num.columns]),\n",
        "        ],\n",
        "        remainder=\"drop\",\n",
        "        verbose_feature_names_out=False\n",
        "    )\n",
        "    gmm = GaussianMixture(\n",
        "        n_components=n_components, covariance_type=covariance_type,\n",
        "        reg_covar=1e-6, n_init=5, init_params=\"kmeans\",\n",
        "        random_state=random_state\n",
        "    )\n",
        "    pipe = Pipeline(steps=[\n",
        "        (\"pre\", ct),\n",
        "        (\"post_std\", StandardScaler(with_mean=True, with_std=True)),\n",
        "        (\"gmm\", gmm)\n",
        "    ])\n",
        "    return pipe\n",
        "\n",
        "# Exploramos k=3,...,8 y recolectar m√©tricas\n",
        "scores, fitted_pipes, labels_dict = [], {}, {}\n",
        "\n",
        "for k in range(3, 9):\n",
        "    pipe = make_gmm_pipeline(n_components=k)\n",
        "    pipe.fit(X_num)\n",
        "\n",
        "    # Datos transformados que usa el GMM (para AIC/BIC)\n",
        "    Xt = pipe.named_steps[\"post_std\"].transform(\n",
        "            pipe.named_steps[\"pre\"].transform(X_num)\n",
        "         )\n",
        "    gmm = pipe.named_steps[\"gmm\"]\n",
        "\n",
        "    aic = gmm.aic(Xt)\n",
        "    bic = gmm.bic(Xt)\n",
        "    ll  = gmm.score(Xt) * Xt.shape[0]   # log-likelihood total\n",
        "    yhat = gmm.predict(Xt)\n",
        "\n",
        "    scores.append(dict(k=k, AIC=aic, BIC=bic, loglik=ll))\n",
        "    fitted_pipes[k] = pipe\n",
        "    labels_dict[k] = yhat\n",
        "\n",
        "gmm_scores_df = pd.DataFrame(scores).sort_values(\"k\").reset_index(drop=True)\n",
        "\n",
        "display(gmm_scores_df)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Un criterio estad√≠stico para elegir $k$ en modelos de mezcla, seleccionar el $k$ que minimiza BIC (o AIC).\n",
        "\n",
        "* $\\mathrm{AIC}=2p-2\\log L$ y $\\mathrm{BIC}=p\\log n-2\\log L$ (BIC penaliza m√°s fuerte).\n",
        "* BIC es consistente: con $n\\to\\infty$ tiende a recuperar el $k$ verdadero.\n",
        "* Si varios $k$ est√°n ‚Äúempatados‚Äù, usar ŒîBIC (Kass‚ÄìRaftery):\n",
        "  ŒîBIC $= \\mathrm{BIC}(k)-\\min_k \\mathrm{BIC}$.\n",
        "  0‚Äì2: evidencia d√©bil; 2‚Äì6: positiva; 6‚Äì10: fuerte; >10: muy fuerte contra ese $k$."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Curvas AIC/BIC vs k\n",
        "crit = gmm_scores_df.melt(id_vars=\"k\", value_vars=[\"AIC\",\"BIC\"],\n",
        "                          var_name=\"criterio\", value_name=\"valor\")\n",
        "fig_ic = px.line(crit, x=\"k\", y=\"valor\", color=\"criterio\", markers=True,\n",
        "                 template=\"plotly_white\", title=\"AIC y BIC vs k (GMM)\")\n",
        "k_star = gmm_scores_df.loc[gmm_scores_df[\"BIC\"].idxmin(),\"k\"]\n",
        "fig_ic.add_vline(x=k_star, line_dash=\"dash\", line_color=\"black\",\n",
        "                 annotation_text=f\"k*={k_star} (min BIC)\")\n",
        "fig_ic.show()\n",
        "\n",
        "# Barras de ŒîBIC para cuantificar evidencia\n",
        "gmm_scores_df[\"dBIC\"] = gmm_scores_df[\"BIC\"] - gmm_scores_df[\"BIC\"].min()\n",
        "fig_dbic = px.bar(gmm_scores_df, x=\"k\", y=\"dBIC\", template=\"plotly_white\",\n",
        "                  title=\"ŒîBIC respecto al m√≠nimo\", labels={\"dBIC\":\"ŒîBIC\"})\n",
        "fig_dbic.add_hline(y=2, line_dash=\"dot\"); fig_dbic.add_hline(y=6, line_dash=\"dot\")\n",
        "fig_dbic.add_hline(y=10, line_dash=\"dot\")\n",
        "fig_dbic.show()\n",
        "\n",
        "# Modelo y etiquetas del k √≥ptimo\n",
        "k_star = int(k_star)\n",
        "best_pipe = fitted_pipes[k_star]\n",
        "yhat_best = labels_dict[k_star]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Notamos que BIC cae mon√≥tonamente y alcanza su m√≠nimo en $k=8$; los ŒîBIC de $k<8$ son muy grandes ‚áí evidencia muy fuerte a favor de $k=8$ dentro del rango evaluado.\n",
        "\n",
        "En conclusi√≥n usar $k^*=\\arg\\min \\mathrm{BIC}$ (aqu√≠, $k^*=8$). Las curvas AIC/BIC y la barra de ŒîBIC sustentan estad√≠sticamente la elecci√≥n y muestran el trade-off ajuste‚Äìcomplejidad."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "cell_id": "dd342e336254418ba766b29dce16b267",
        "deepnote_cell_type": "markdown",
        "id": "P9CERnaerqDC"
      },
      "source": [
        "## 6. An√°lisis de resultados üìä [10 puntos]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "cell_id": "953b5ad01a704b50b899db7176d1b7b2",
        "deepnote_cell_type": "markdown",
        "id": "I1yNa111rqDC"
      },
      "source": [
        "<center>\n",
        "<img src=\"https://i.pinimg.com/originals/5b/03/4e/5b034e96d84c6c6b57a9a04ca14aac02.gif\" width=400>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "cell_id": "fd90e2f135404353ac0b5ab844936ca7",
        "deepnote_cell_type": "markdown",
        "id": "dg0Qx4RZrqDC"
      },
      "source": [
        "Una vez identificado el n√∫mero √≥ptimo de cl√∫sters, se le pide realizar lo siguiente:\n",
        "\n",
        "1. Utilizar la proyecci√≥n en dos dimensiones para visualizar cada cl√∫ster claramente. [2 puntos]\n",
        "\n",
        "2. ¬øEs posible distinguir claramente entre los cl√∫sters generados? [2 puntos]\n",
        "\n",
        "3. Proporcionar una descripci√≥n breve de cada cl√∫ster utilizando estad√≠sticas descriptivas b√°sicas, como la media y la desviaci√≥n est√°ndar, para resumir las caracter√≠sticas de las variables utilizadas en estos algoritmos. [2 puntos]\n",
        "\n",
        "4. Proceda a visualizar los cl√∫sters en tres dimensiones para una perspectiva m√°s detallada. [2 puntos]\n",
        "\n",
        "5. ¬øC√≥mo afecta esto a sus conclusiones anteriores? [2 puntos]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CRN0zZip0IMB"
      },
      "source": [
        "**Respuestas:**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cell_id": "9abf4dbc643e40cebe99fcb1ff3ff413",
        "deepnote_cell_type": "code",
        "id": "XmZrz15GrqDC"
      },
      "outputs": [],
      "source": [
        "# PUNTO 1: Cl√∫sters en 2D (PCA)\n",
        "\n",
        "yhat_best = labels_dict[k_star]                  # asignaciones del GMM √≥ptimo\n",
        "plot2d = X_pca2_df.copy()\n",
        "plot2d[\"cluster\"] = pd.Categorical(yhat_best)    # para colores consistentes\n",
        "\n",
        "# Centros en el plano (promedio de PC1, PC2 por cluster)\n",
        "centros_2d = plot2d.groupby(\"cluster\")[[\"PC1\",\"PC2\"]].mean().reset_index()\n",
        "sizes = plot2d[\"cluster\"].value_counts().sort_index()\n",
        "centros_2d[\"n\"] = centros_2d[\"cluster\"].map(sizes)\n",
        "\n",
        "# Scatter 2D con Plotly: puntos por cluster + centros (s√≠mbolo 'x')\n",
        "fig_clusters_2d = px.scatter(\n",
        "    plot2d, x=\"PC1\", y=\"PC2\", color=\"cluster\",\n",
        "    opacity=0.55, render_mode=\"webgl\",\n",
        "    title=f\"Cl√∫sters GMM en PCA 2D (k={k_star})\",\n",
        "    labels={\"cluster\":\"Cluster\"}\n",
        ")\n",
        "\n",
        "# Centros\n",
        "fig_clusters_2d.add_trace(go.Scatter(\n",
        "    x=centros_2d[\"PC1\"], y=centros_2d[\"PC2\"], mode=\"markers+text\",\n",
        "    marker=dict(symbol=\"x\", size=16, color=\"black\"),\n",
        "    text=[f\"C{c} (n={n})\" for c,n in zip(centros_2d[\"cluster\"], centros_2d[\"n\"])],\n",
        "    textposition=\"top center\", name=\"Centros\"\n",
        "))\n",
        "\n",
        "fig_clusters_2d.update_layout(template=\"plotly_white\", width=900, height=650, legend_title_text=\"Cluster\")\n",
        "fig_clusters_2d.update_xaxes(showgrid=False, showline=True, linecolor=\"black\", mirror=True, ticks=\"outside\")\n",
        "fig_clusters_2d.update_yaxes(showgrid=False, showline=True, linecolor=\"black\", mirror=True, ticks=\"outside\")\n",
        "fig_clusters_2d.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "¬øSe distinguen claramente los cl√∫sters?\n",
        "\n",
        "Parcialmente. El mapa PCA-2D muestra:\n",
        "\n",
        "* Estructura: hay gradientes y ‚Äúzonas‚Äù (p.ej., grupos que ocupan regiones altas en PC2 o derechas en PC1), y los centros (‚úñ) est√°n bien ordenados.\n",
        "* Solapamiento: la regi√≥n central y las bandas (por la naturaleza Likert) presentan mezcla intensa de colores; en 2D varios cl√∫sters se interpenetran. Esto es esperable en GMM, que modela componentes solapadas con asignaciones probabil√≠sticas.\n",
        "\n",
        "C√≥mo podemos leer esto?:\n",
        "\n",
        "* Los cl√∫sters son identificables como tendencias (√°reas predominantes), pero no separables linealmente ni ‚Äúa ojo‚Äù en todo el plano.\n",
        "* Para decisiones de marketing o del negocio, conviene trabajar con pertenencias suaves: usar la probabilidad posterior del GMM; considerar ‚Äúmiembros seguros‚Äù con $\\max_j \\Pr(z=j\\mid x) \\ge 0.7$ y tratar el resto como frontera o mixtos."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Para el punto 3 lo que vamos a hacer es:\n",
        "\n",
        "1. Calcular media, desviaci√≥n est√°ndar y tama√±o por cl√∫ster.\n",
        "2. Calcula el tama√±o del efecto por variable y cl√∫ster\n",
        "\n",
        "$$\n",
        "\\text{ES}_{c,v}=\\frac{\\mu_{c,v}-\\mu_{global,v}}{\\sigma_{global,v}}\n",
        "$$\n",
        "\n",
        "(algo as√≠ como ‚Äúcu√°ntas sigmas por sobre/bajo el promedio global‚Äù)\n",
        "\n",
        "> Nota: las estad√≠sticas las vamos a reportar en la escala original (`X_num`), no en la transformada, para que sean interpretables."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# PUNTO 3\n",
        "# Resumen descriptivo por cl√∫ster (media, std) + tama√±os de efecto\n",
        "\n",
        "# Variables a resumir: usamos las 4 seleccionadas\n",
        "vars_show = list(seleccion)\n",
        "\n",
        "df_lab = X_num.copy()\n",
        "df_lab[\"cluster\"] = yhat_best\n",
        "\n",
        "# Estad√≠sticas b√°sicas por cl√∫ster\n",
        "desc = (\n",
        "    df_lab.groupby(\"cluster\")[vars_show]\n",
        "          .agg([\"mean\",\"std\",\"count\"])\n",
        "          .sort_index()\n",
        ")\n",
        "display(desc)\n",
        "\n",
        "# Tama√±o del efecto (mean_cluster - mean_global)/std_global\n",
        "mu_g = X_num[vars_show].mean()\n",
        "sd_g = X_num[vars_show].std(ddof=0)  # poblaci√≥n para comparar\n",
        "\n",
        "es = (\n",
        "    df_lab.groupby(\"cluster\")[vars_show].mean()\n",
        "          .sub(mu_g, axis=1).div(sd_g, axis=1)\n",
        "          .sort_index()\n",
        ")\n",
        "es_rounded = es.round(2)\n",
        "display(es_rounded)\n",
        "\n",
        "# Heatmap de tama√±os de efecto para visualizar \"alto/bajo\"\n",
        "fig_es = px.imshow(\n",
        "    es, color_continuous_scale=\"RdBu\", zmin=-1.5, zmax=1.5,\n",
        "    labels=dict(color=\"ES (sigmas)\"),\n",
        "    title=\"Tama√±os de efecto por cl√∫ster (media vs global, en sigmas)\"\n",
        ")\n",
        "fig_es.update_layout(template=\"plotly_white\", width=900, height=350+30*len(es))\n",
        "fig_es.show()\n",
        "\n",
        "# Descripci√≥n breve autom√°tica por cl√∫ster\n",
        "def describe_cluster(es_row, top=4, thr=0.35):\n",
        "    \"\"\"Devuelve texto con variables m√°s altas/bajas (|ES|>=thr), hasta 'top' por lado.\"\"\"\n",
        "    highs = es_row[es_row>=thr].sort_values(ascending=False).head(top).index.tolist()\n",
        "    lows  = es_row[es_row<=-thr].sort_values(ascending=True ).head(top).index.tolist()\n",
        "    parts = []\n",
        "    if highs: parts.append(\"altas: \" + \", \".join(highs))\n",
        "    if lows:  parts.append(\"bajas: \" + \", \".join(lows))\n",
        "    return \"; \".join(parts) if parts else \"perfil balanceado\"\n",
        "\n",
        "resumen = (\n",
        "    es.apply(lambda r: describe_cluster(r, top=4, thr=0.35), axis=1)\n",
        "      .to_frame(\"descripcion_breve\")\n",
        ")\n",
        "resumen[\"n\"] = df_lab[\"cluster\"].value_counts().sort_index()\n",
        "display(resumen)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Ahora como se nos pide en el enunciado, hacemos una interpretaci√≥n por cl√∫ster (variables: Inflight entertainment, Online boarding, Arrival Delay \\[min], Flight Distance). Entre par√©ntesis se indica el tama√±o del efecto (ES) vs. la media global ‚âà ‚Äúcu√°ntas sigmas‚Äù arriba/abajo y el n del grupo.\n",
        "\n",
        "* **C0 (n=12 586)** ‚Äì Largo y digital alto, retrasos normales.\n",
        "  OB alto (+0.53), IE alto (+0.42), Distancia alta (+0.53), Delay \\~0.\n",
        "\n",
        "* **C1 (n=7 790)** ‚Äì Retrasadores cr√≥nicos de vuelos largos.\n",
        "  Delay muy alto (+2.84), Distancia alta (+0.50), IE/OB levemente bajos (‚àí0.17/‚àí0.05).\n",
        "\n",
        "* **C2 (n=8 042)** ‚Äì Perfil medio-digital.\n",
        "  OB algo alto (+0.32), IE levemente alto (+0.12), Distancia y Delay cercanos a 0.\n",
        "\n",
        "* **C3 (n=26 656)** ‚Äì Corto recorrido y digital bajo.\n",
        "  OB bajo (‚àí0.60), Distancia baja (‚àí0.48), IE levemente bajo (‚àí0.14), Delay \\~0.\n",
        "\n",
        "* **C4 (n=41 842)** ‚Äì Mixto, ligeramente digital.\n",
        "  OB algo alto (+0.28), IE levemente bajo (‚àí0.13), Distancia y Delay \\~0.\n",
        "\n",
        "* **C5 (n=45 338)** ‚Äì ‚ÄúPremium eficiente‚Äù en vuelos largos.\n",
        "  OB alto (+0.65), IE alto (+0.56), Distancia alta (+0.64), Delay bajo (‚àí0.14).\n",
        "\n",
        "* **C6 (n=91 640; mayoritario)** ‚Äì Corto recorrido, baja experiencia digital/IE.\n",
        "  OB bajo (‚àí0.37), IE bajo (‚àí0.23), Distancia baja (‚àí0.37), Delay levemente bajo (‚àí0.09).\n",
        "\n",
        "* **C7 (n=2 220; minoritario)** ‚Äì Perfil balanceado.\n",
        "  ES cercanos a 0; sin rasgos marcados.\n",
        "\n",
        "Interpretaci√≥n:\n",
        "\n",
        "* Los cl√∫sters se organizan en dos ejes claros: longitud del vuelo (corto ‚Üî largo) y experiencia digital/entretenimiento, con el retraso destacando en C1 (pico de Delay) y siendo menor en C5/C6."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# PUNTO 4\n",
        "# Visualizaci√≥n 3D de cl√∫sters GMM sobre PCA (3 componentes)\n",
        "\n",
        "# k* (m√≠nimo BIC) y etiquetas\n",
        "yhat_best = labels_dict[k_star]\n",
        "best_pipe = fitted_pipes[k_star]\n",
        "\n",
        "# Proyecci√≥n PCA a 3D (mismo pipeline de Secci√≥n 3)\n",
        "pca3_pipe = make_pca_pipeline(selected_cols=locals().get(\"seleccion\", None),\n",
        "                              whiten=False, random_state=42)\n",
        "pca3_pipe.set_params(pca__n_components=3)\n",
        "X_pca3 = pca3_pipe.fit_transform(X_num)\n",
        "vr3 = pca3_pipe.named_steps[\"pca\"].explained_variance_ratio_\n",
        "\n",
        "pca3_df = pd.DataFrame(X_pca3, columns=[\"PC1\",\"PC2\",\"PC3\"], index=X_num.index)\n",
        "pca3_df[\"cluster\"] = pd.Categorical(yhat_best)\n",
        "\n",
        "# Probabilidades de pertenencia y m√°scaras de confianza\n",
        "Z = best_pipe.named_steps[\"post_std\"].transform(\n",
        "        best_pipe.named_steps[\"pre\"].transform(X_num)\n",
        ")\n",
        "probs = best_pipe.named_steps[\"gmm\"].predict_proba(Z)\n",
        "pca3_df[\"max_prob\"] = probs.max(axis=1)\n",
        "\n",
        "thr_conf = 0.70\n",
        "hi = pca3_df.query(\"max_prob >= @thr_conf\").copy()   # alta confianza\n",
        "lo = pca3_df.query(\"max_prob <  @thr_conf\").copy()   # baja confianza\n",
        "\n",
        "# Submuestreo opcional para reducir oclusi√≥n (baja confianza)\n",
        "n_lo = min(len(lo), 20_000)\n",
        "lo = lo.sample(n=n_lo, random_state=42) if len(lo) > n_lo else lo\n",
        "\n",
        "# Centros y elipsoides 1œÉ por cl√∫ster en PCA3\n",
        "def ellipsoid_wireframe(mu, Sigma, n_u=22, n_v=14, scale=1.0):\n",
        "    \"\"\"Devuelve malla (x,y,z) de elipsoide 1œÉ (wireframe/surface).\"\"\"\n",
        "    evals, evecs = np.linalg.eigh(Sigma)\n",
        "    radii = np.sqrt(np.maximum(evals, 1e-12)) * scale\n",
        "    u = np.linspace(0, 2*np.pi, n_u)\n",
        "    v = np.linspace(0, np.pi, n_v)\n",
        "    xs = np.outer(np.cos(u), np.sin(v))\n",
        "    ys = np.outer(np.sin(u), np.sin(v))\n",
        "    zs = np.outer(np.ones_like(u), np.cos(v))\n",
        "    E = evecs @ np.diag(radii)\n",
        "    X = E @ np.vstack([xs.ravel(), ys.ravel(), zs.ravel()])\n",
        "    X = X.T.reshape(xs.shape[0], xs.shape[1], 3) + mu\n",
        "    return dict(x=X[:, :, 0], y=X[:, :, 1], z=X[:, :, 2])\n",
        "\n",
        "centros = pca3_df.groupby(\"cluster\", observed=True)[[\"PC1\",\"PC2\",\"PC3\"]].mean()\n",
        "covs = {\n",
        "    c: pca3_df.loc[pca3_df[\"cluster\"] == c, [\"PC1\",\"PC2\",\"PC3\"]].cov().values\n",
        "    for c in centros.index\n",
        "}\n",
        "\n",
        "# Figura 3D mejorada\n",
        "fig = go.Figure()\n",
        "\n",
        "# Baja confianza (gris tenue, al fondo)\n",
        "fig.add_trace(go.Scatter3d(\n",
        "    x=lo[\"PC1\"], y=lo[\"PC2\"], z=lo[\"PC3\"], mode=\"markers\",\n",
        "    marker=dict(size=2, color=\"rgba(120,120,120,0.18)\"),\n",
        "    name=\"Baja confianza\", hoverinfo=\"skip\"\n",
        "))\n",
        "\n",
        "# Alta confianza por cl√∫ster + elipsoides 1œÉ\n",
        "palette = px.colors.qualitative.Plotly\n",
        "for idx, (c, dfc) in enumerate(hi.groupby(\"cluster\", observed=True)):\n",
        "    color = palette[idx % len(palette)]\n",
        "    fig.add_trace(go.Scatter3d(\n",
        "        x=dfc[\"PC1\"], y=dfc[\"PC2\"], z=dfc[\"PC3\"], mode=\"markers\",\n",
        "        marker=dict(size=3, opacity=0.35, color=color),\n",
        "        name=f\"Cluster {c}\", legendgroup=f\"C{c}\", showlegend=True\n",
        "    ))\n",
        "    mu = centros.loc[c].values\n",
        "    Sigma = covs[c]\n",
        "    wire = ellipsoid_wireframe(mu, Sigma, n_u=22, n_v=14, scale=1.0)\n",
        "    fig.add_trace(go.Surface(\n",
        "        x=wire[\"x\"], y=wire[\"y\"], z=wire[\"z\"],\n",
        "        showscale=False, opacity=0.12,\n",
        "        name=f\"Elipsoide C{c}\", legendgroup=f\"C{c}\", showlegend=False,\n",
        "        colorscale=[[0, color], [1, color]]\n",
        "    ))\n",
        "\n",
        "# Centros (X negras)\n",
        "fig.add_trace(go.Scatter3d(\n",
        "    x=centros[\"PC1\"], y=centros[\"PC2\"], z=centros[\"PC3\"],\n",
        "    mode=\"markers+text\", marker=dict(symbol=\"x\", size=8, color=\"black\"),\n",
        "    text=[f\"C{c}\" for c in centros.index], textposition=\"top center\",\n",
        "    name=\"Centros\"\n",
        "))\n",
        "\n",
        "fig.update_layout(\n",
        "    title=(f\"GMM en PCA 3D (k={k_star}) ‚Äî Var. explicada acumulada: \"\n",
        "           f\"{vr3.cumsum()[2]:.1%}  |  Umbral confianza ‚â• {thr_conf:.0%}\"),\n",
        "    template=\"plotly_white\", width=950, height=700, legend_title_text=\"\",\n",
        "    scene_camera=dict(eye=dict(x=1.6, y=1.8, z=1.2)),\n",
        "    scene=dict(\n",
        "        xaxis_title=\"PC1\", yaxis_title=\"PC2\", zaxis_title=\"PC3\",\n",
        "        xaxis=dict(showspikes=False), yaxis=dict(showspikes=False), zaxis=dict(showspikes=False)\n",
        "    )\n",
        ")\n",
        "fig.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "6. ¬øC√≥mo afecta el 3D a las conclusiones anteriores?\n",
        "\n",
        "Primero en la elecci√≥n de $k$:\n",
        "\n",
        "* En 2D ya concluimos $k^*=8$ por BIC m√≠nimo.\n",
        "* La proyecci√≥n 3D (‚âà82‚Äì83% de varianza) no contradice esa decisi√≥n: los n√∫cleos de varios cl√∫sters aparecen mejor definidos cuando filtramos por alta confianza; no se observan fusiones claras que sugieran $k$ menor, ni subestructuras evidentes que pidan $k$ mayor dentro del rango 3‚Äì8. Por lo tanto, la selecci√≥n $k=8$ sigue sustantiva y estad√≠sticamente respaldada.\n",
        "\n",
        "Segundo, implicancias pr√°cticas:\n",
        "\n",
        "* El 3D no cambia las conclusiones cuantitativas (AIC/BIC) pero mejora la narrativa: identifica zonas ‚Äúcore‚Äù para acciones focalizadas (p. ej., premium largos vs. rutas cortas low-digital), y zonas ‚Äúfrontera‚Äù para campa√±as de conversi√≥n (elevar adopci√≥n de online boarding).\n",
        "* Confirma que retraso y distancia siguen siendo ejes principales, y que la experiencia digital/entretenimiento diferencia cl√∫sters dentro de cada rango de distancia.\n",
        "\n",
        "Conclusi√≥n final: La visualizaci√≥n 3D refuerza y no revierte las conclusiones de la Secci√≥n 5: $k=8$ es adecuado seg√∫n BIC; los cl√∫sters son interpretables y √∫tiles si se gestionan con soft assignments, y el 3D aporta claridad para comunicar y ejecutar acciones por grupo."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8igIgDwpq9mG"
      },
      "source": [
        "Mucho √©xito!\n",
        "\n",
        "<center>\n",
        "<img src=\"https://i.pinimg.com/originals/55/3d/42/553d42bea9b10e0662a05aa8726fc7f4.gif\" width=300>"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "deepnote_execution_queue": [],
    "deepnote_notebook_id": "7cb425aec99b4079954fd707109c42c3",
    "deepnote_persisted_session": {
      "createdAt": "2024-04-26T06:15:51.197Z"
    },
    "kernelspec": {
      "display_name": "base",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.7"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
