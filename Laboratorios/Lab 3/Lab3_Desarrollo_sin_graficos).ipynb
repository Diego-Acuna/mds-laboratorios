{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "cell_id": "8dd9b572c805487a9fb430fdc4ab12bb",
        "deepnote_cell_height": 156.26666259765625,
        "deepnote_cell_type": "markdown",
        "id": "XUZ1dFPHzAHl"
      },
      "source": [
        "<h1><center>Laboratorio 3: La desperaci√≥n de Mr. Cheems üêº</center></h1>\n",
        "\n",
        "<center><strong>MDS7202: Laboratorio de Programaci√≥n Cient√≠fica para Ciencia de Datos - Primavera 2025</strong></center>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "cell_id": "d65413cd8566460dbceffcd13ca236e7",
        "deepnote_cell_type": "markdown",
        "id": "UD8X1uhGzAHq"
      },
      "source": [
        "### Cuerpo Docente:\n",
        "\n",
        "- Profesores: Diego Cortez, Gabriel Iturra\n",
        "- Auxiliares: Melanie Pe√±a, Valentina Rojas\n",
        "- Ayudantes: Nicol√°s Cabello, Cristopher Urbina"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "cell_id": "8e9217d02d124830a9b86046600a1605",
        "deepnote_cell_height": 172.13333129882812,
        "deepnote_cell_type": "markdown",
        "id": "tXflExjqzAHr"
      },
      "source": [
        "### Equipo: SUPER IMPORTANTE - notebooks sin nombre no ser√°n revisados\n",
        "\n",
        "- Nombre de alumno 1: Tom√°s Ram√≠rez\n",
        "- Nombre de alumno 2: Diego Acu√±a\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "cell_id": "010402b6d5f743b885a80d2e1c6ae11a",
        "deepnote_cell_height": 62.19999694824219,
        "deepnote_cell_type": "markdown",
        "id": "AD-V0bbZzAHr"
      },
      "source": [
        "### **Link de repositorio de GitHub:** [Repositorio de GitHub](https://github.com/Diego-Acuna/mds-laboratorios)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "cell_id": "ef0224c7a99e4b718b55493b0a1e99c4",
        "deepnote_cell_height": 724.9000244140625,
        "deepnote_cell_type": "markdown",
        "id": "6uBLPj1PzAHs"
      },
      "source": [
        "## Temas a tratar\n",
        "- Aplicar Pandas para obtener caracter√≠sticas de un DataFrame.\n",
        "- Aplicar Pipelines y Column Transformers\n",
        "\n",
        "## Reglas:\n",
        "\n",
        "- **Grupos de 2 personas**\n",
        "- Fecha de entrega: Entregas Martes a las 23:59.\n",
        "- Instrucciones del lab el viernes a las 16:15 en formato online. Asistencia no es obligatoria, pero se recomienda **fuertemente** asistir.\n",
        "- <u>Prohibidas las copias</u>. Cualquier intento de copia ser√° debidamente penalizado con el reglamento de la escuela.\n",
        "- Tienen que subir el laboratorio a u-cursos y a su repositorio de github. Labs que no est√©n en u-cursos no ser√°n revisados. Recuerden que el repositorio tambi√©n tiene nota.\n",
        "- Cualquier duda fuera del horario de clases al foro. Mensajes al equipo docente ser√°n respondidos por este medio.\n",
        "- Pueden usar cualquier material del curso que estimen conveniente.\n",
        "\n",
        "### Objetivos principales del laboratorio\n",
        "- Comprender c√≥mo aplicar pipelines de Scikit-Learn para generar procesos m√°s limpios en Feature Engineering.\n",
        "\n",
        "El laboratorio deber√° ser desarrollado sin el uso indiscriminado de iteradores nativos de python (aka \"for\", \"while\"). La idea es que aprendan a exprimir al m√°ximo las funciones optimizadas que nos entrega `numpy`, las cuales vale mencionar, son bastante m√°s eficientes que los iteradores nativos sobre arreglos (*o tensores*)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "cell_id": "59664481c26f4ac4a753765269b1db6a",
        "deepnote_cell_height": 69.86666870117188,
        "deepnote_cell_type": "markdown",
        "id": "wrG4gYabzAHs"
      },
      "source": [
        "## Descripci√≥n del laboratorio."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "cell_id": "8c7bf8ea553d44c7a2efd61106a0bac2",
        "deepnote_cell_height": 61.866668701171875,
        "deepnote_cell_type": "markdown",
        "id": "MhISwri4zAHy"
      },
      "source": [
        "### Importamos librerias utiles üò∏"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2021-03-29T00:08:16.884674Z",
          "start_time": "2021-03-29T00:08:16.349846Z"
        },
        "cell_id": "67b4b29f0e6b48719b58d579276f2b19",
        "deepnote_cell_height": 514.13330078125,
        "deepnote_cell_type": "code",
        "deepnote_to_be_reexecuted": false,
        "execution_millis": 8517,
        "execution_start": 1635469788590,
        "id": "uyc33dKdzAHy",
        "source_hash": "a3741fd5"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: plotly in /opt/anaconda3/lib/python3.12/site-packages (6.3.0)\n",
            "Requirement already satisfied: narwhals>=1.15.1 in /opt/anaconda3/lib/python3.12/site-packages (from plotly) (1.31.0)\n",
            "Requirement already satisfied: packaging in /opt/anaconda3/lib/python3.12/site-packages (from plotly) (24.1)\n"
          ]
        }
      ],
      "source": [
        "# Libreria Core del lab.\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import datetime\n",
        "from IPython.display import HTML\n",
        "\n",
        "# Libreria para plotear (En colab esta desactualizado plotly)\n",
        "!pip install --upgrade plotly\n",
        "import plotly.express as px\n",
        "import plotly.graph_objects as go\n",
        "\n",
        "# Librerias utiles\n",
        "from sklearn.base import BaseEstimator, TransformerMixin\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.compose import ColumnTransformer\n",
        "from sklearn.preprocessing import FunctionTransformer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "cell_id": "ce6a19ec6fc6486e832760ac3740d7ef",
        "deepnote_cell_height": 219.46665954589844,
        "deepnote_cell_type": "code",
        "deepnote_to_be_reexecuted": false,
        "execution_millis": 7,
        "execution_start": 1635165625274,
        "id": "gQ0-zPV4NNrq",
        "outputId": "a7c33afa-37fe-4965-de1a-53b8994c8c07",
        "source_hash": "c60dc4a7"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Ignorando conexi√≥n drive-colab\n"
          ]
        }
      ],
      "source": [
        "# Si usted est√° utilizando Colabolatory le puede ser √∫til este c√≥digo para cargar los archivos.\n",
        "try:\n",
        "    from google.colab import drive\n",
        "    drive.mount(\"/content/drive\")\n",
        "    path = 'Direcci√≥n donde tiene los archivos en el Drive'\n",
        "except:\n",
        "    print('Ignorando conexi√≥n drive-colab')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "cell_id": "28c7a8b483d84878ac5a4f7ba882b711",
        "deepnote_cell_height": 133.86666870117188,
        "deepnote_cell_type": "markdown",
        "id": "QDwIXTh7bK_A",
        "owner_user_id": "badcc427-fd3d-4615-9296-faa43ec69cfb"
      },
      "source": [
        "# Feature engineering en datos de retail üõçÔ∏è"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "cell_id": "160bb2695f6547448bfb0f99420f952c",
        "deepnote_cell_height": 69.86666870117188,
        "deepnote_cell_type": "markdown",
        "id": "_Eu4qBqnXMff",
        "tags": []
      },
      "source": [
        "### 0. Cargar Dataset"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "cell_id": "6c6799ecc9e74272922d46a3b5a8b79e",
        "deepnote_cell_height": 294.683349609375,
        "deepnote_cell_type": "markdown",
        "id": "4shIzqqwXMfe",
        "tags": []
      },
      "source": [
        "<p align=\"center\">\n",
        "  <img width=300 src=\"https://s1.eestatic.com/2018/04/14/social/la_jungla_-_social_299733421_73842361_854x640.jpg\">\n",
        "</p>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "cell_id": "48d29c89e3b6455083f8fac764f97f3b",
        "deepnote_cell_height": 475.066650390625,
        "deepnote_cell_type": "markdown",
        "id": "cDpKjYRCXMfg",
        "tags": []
      },
      "source": [
        "Mr. Cheems, gerente de una cotizada tienda de retail en Europa, les solicita si pueden analizar los datos de algunas de sus tiendas. En una reuni√≥n, Mr Cheems le comenta que la calidad de sus datos no es muy buena, por lo que le solicita a usted que limpie su base de datos y cree nuevos atributos relevantes para el negocio.\n",
        "\n",
        "Por ello, el √°rea de ventas les entrega archivo llamado `online_retail_data.pickle` el cual usted decide cargar a continuaci√≥n."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "cell_id": "4d7d0f0855744e6c9d5a2198e5dcd690",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        },
        "deepnote_cell_height": 489.79998779296875,
        "deepnote_cell_type": "code",
        "deepnote_output_heights": [
          177
        ],
        "deepnote_to_be_reexecuted": false,
        "execution_millis": 466,
        "execution_start": 1635469797118,
        "id": "7FNOu-CvjV5m",
        "outputId": "90b4f92c-71df-44d4-8084-4dd06a6179e4",
        "source_hash": "d52b246c"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Invoice</th>\n",
              "      <th>StockCode</th>\n",
              "      <th>Description</th>\n",
              "      <th>Quantity</th>\n",
              "      <th>InvoiceDate</th>\n",
              "      <th>Price</th>\n",
              "      <th>Customer ID</th>\n",
              "      <th>Country</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>ID489434</td>\n",
              "      <td>85048</td>\n",
              "      <td>15CM CHRISTMAS GLASS BALL 20 LIGHTS</td>\n",
              "      <td>12.0</td>\n",
              "      <td>2009-12-01 07:45:00</td>\n",
              "      <td>6.95</td>\n",
              "      <td>13085.0</td>\n",
              "      <td>United Kingdom</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>ID489434</td>\n",
              "      <td>79323P</td>\n",
              "      <td>PINK CHERRY LIGHTS</td>\n",
              "      <td>12.0</td>\n",
              "      <td>NaT</td>\n",
              "      <td>6.75</td>\n",
              "      <td>13085.0</td>\n",
              "      <td>United Kingdom</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>ID489434</td>\n",
              "      <td>79323W</td>\n",
              "      <td>WHITE CHERRY LIGHTS</td>\n",
              "      <td>12.0</td>\n",
              "      <td>2009-12-01 07:45:00</td>\n",
              "      <td>6.75</td>\n",
              "      <td>13085.0</td>\n",
              "      <td>United Kingdom</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>ID489434</td>\n",
              "      <td>22041</td>\n",
              "      <td>RECORD FRAME 7\" SINGLE SIZE</td>\n",
              "      <td>48.0</td>\n",
              "      <td>2009-12-01 07:45:00</td>\n",
              "      <td>2.10</td>\n",
              "      <td>13085.0</td>\n",
              "      <td>United Kingdom</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>ID489434</td>\n",
              "      <td>21232</td>\n",
              "      <td>STRAWBERRY CERAMIC TRINKET BOX</td>\n",
              "      <td>24.0</td>\n",
              "      <td>2009-12-01 07:45:00</td>\n",
              "      <td>1.25</td>\n",
              "      <td>13085.0</td>\n",
              "      <td>United Kingdom</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "    Invoice StockCode                          Description  Quantity  \\\n",
              "0  ID489434     85048  15CM CHRISTMAS GLASS BALL 20 LIGHTS      12.0   \n",
              "1  ID489434    79323P                   PINK CHERRY LIGHTS      12.0   \n",
              "2  ID489434    79323W                  WHITE CHERRY LIGHTS      12.0   \n",
              "3  ID489434     22041         RECORD FRAME 7\" SINGLE SIZE       48.0   \n",
              "4  ID489434     21232       STRAWBERRY CERAMIC TRINKET BOX      24.0   \n",
              "\n",
              "          InvoiceDate  Price Customer ID         Country  \n",
              "0 2009-12-01 07:45:00   6.95     13085.0  United Kingdom  \n",
              "1                 NaT   6.75     13085.0  United Kingdom  \n",
              "2 2009-12-01 07:45:00   6.75     13085.0  United Kingdom  \n",
              "3 2009-12-01 07:45:00   2.10     13085.0  United Kingdom  \n",
              "4 2009-12-01 07:45:00   1.25     13085.0  United Kingdom  "
            ]
          },
          "execution_count": 3,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Inserte su c√≥digo aqu√≠\n",
        "df_retail = pd.read_pickle(\"online_retail_data.pickle\")\n",
        "df_retail.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Q6nm_0uWvrFv"
      },
      "source": [
        "### 1. Funci√≥n para explorar caracter√≠sticas [0.5 puntos]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KOZEZbbLoqfI"
      },
      "source": [
        "<p align=\"center\">\n",
        "  <img width=300 src=\"https://editor.analyticsvidhya.com/uploads/47389meme.png\">\n",
        "</p>\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "J-7ZaNutk2GO"
      },
      "source": [
        "\n",
        "\n",
        "Tras inspeccionar brevemente los datos proporcionados, usted decide crear una funci√≥n que realice lo siguiente:\n",
        "- Plotee un histograma para las variables precios y cantidad. [0.3 puntos]\n",
        "- Imprima un conteo de datos nulos por variable [0.2 puntos]\n",
        "\n",
        "**Nota**: Para generar los gr√°ficos no es obligatorio el uso de `plotly`, pero si es altamente recomendado. Pueden encontrar m√°s informaci√≥n de esta librer√≠a en este [enlace](https://plotly.com/python/)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TM8FZ_4Yuiwi"
      },
      "source": [
        "**Respuesta:**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "uDqT1Ljpk7vp"
      },
      "outputs": [],
      "source": [
        "def explore_data(dataframe_in):\n",
        "    \"\"\"\n",
        "    Explora r√°pidamente el dataset:\n",
        "      - Histogramas de 'Price' y 'Quantity' con Plotly.\n",
        "      - Conteo de valores nulos por columna (impreso y retornado).\n",
        "\n",
        "    Par√°metros\n",
        "    ----------\n",
        "    dataframe_in : pd.DataFrame\n",
        "        DataFrame con las columnas 'Price' y 'Quantity'.\n",
        "    \"\"\"\n",
        "    # Hacemos el histograma para la variable 'Quantity'\n",
        "    fig_qty = px.histogram(\n",
        "        dataframe_in,\n",
        "        x=\"Quantity\",\n",
        "        nbins=60,\n",
        "        title=\"Distribuci√≥n de Quantity\",\n",
        "    )\n",
        "    \n",
        "    # Mostramos el histograma en pantalla\n",
        "    fig_qty.update_layout(bargap=0.01)\n",
        "    fig_qty.show()\n",
        "\n",
        "    # Hacemos el histograma para la variable 'Price'\n",
        "    fig_price = px.histogram(\n",
        "        dataframe_in,\n",
        "        x=\"Price\",\n",
        "        nbins=60,\n",
        "        title=\"Distribuci√≥n de Price\",\n",
        "    )\n",
        "    \n",
        "    # Mostramos el histograma en pantalla\n",
        "    fig_price.update_layout(bargap=0.01)\n",
        "    fig_price.show()\n",
        "\n",
        "    # Contamos la cantidad de datos nulos y los ordenamos por variable\n",
        "    null_counts = dataframe_in.isna().sum().sort_values(ascending=False)\n",
        "    \n",
        "    # Mostramos en pantalla la cantidad de datos nulos por variable\n",
        "    print(\"Conteo de datos nulos por variable:\")\n",
        "    print(null_counts)\n",
        "\n",
        "    # Retornamos la cantidad de datos nulos por variable\n",
        "    return null_counts"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Mostramos la distribuci√≥n de la cantidad y el precio, al igual que la cantidad de datos nulos por variable del dataframe  del retail\n",
        "_ = explore_data(df_retail)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "F4ZY_N0Ad1GP"
      },
      "source": [
        "### 2. Eliminando outliers [1.0 puntos]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yXTpIi1Bo2KG"
      },
      "source": [
        "<p align=\"center\">\n",
        "  <img width=300 src=\"https://media.licdn.com/dms/image/C5612AQGdXKCka7HumA/article-cover_image-shrink_600_2000/0/1520056407281?e=2147483647&v=beta&t=VZcfjjzjK4LxXdZkSu1KisWC0Ry8bk4tPCn3R8aYdNM\">\n",
        "</p>\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ECqH4t-Jvj05"
      },
      "source": [
        "#### 2.1 Creando la clase IQR [0.5 puntos]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GtCQGHN_mzEp"
      },
      "source": [
        "Entre las falencias de los datos, Mr. Cheems le comenta que a veces los operadores no ingresan el precio correcto de los productos. Mr. Cheems le comenta que se dio cuenta de este fen√≥meno porque hay productos con precios exager√°damente altos o bajos. Por lo cual usted decide eliminar outliers del dataframe a traves del rango intercuartil el cual cuenta con los siguientes pasos:\n",
        "\n",
        "1. Calcular el primer cuartil $Q1$ y el tercer cuartil $Q3$. Hint: utilice el m√©todo `quantile()`\n",
        "\n",
        "2. Calcular el rango intercuartil (RIC): $RIC = Q3 - Q1$\n",
        "\n",
        "3. Calcular los l√≠mites para identificar outliers:\n",
        " - L√≠mite inferior: $~~Q1 - \\lambda \\cdot RIC$\n",
        " - L√≠mite superior: $~~Q3 + \\lambda \\cdot RIC$\n",
        "\n",
        "4. Eliminar outliers: Los outliers son los datos que est√°n por debajo del l√≠mite inferior o por encima del l√≠mite superior.\n",
        "\n",
        "\n",
        "Para realizar dicha tarea, usted decide crear una clase llamada `IQR()` utilizando `BaseEstimator` y `TransformerMixin` para realizar una transformaci√≥n de cada una de las columnas num√©ricas del DataFrame utilizando `ColumnTransformer()` m√°s tarde. Considere que lambda debe ser $\\lambda$ un par√°metro a definir por el usuario.\n",
        "\n",
        "**Hint:** tome como referencia el siguiente [enlace](https://sklearn-template.readthedocs.io/en/latest/user_guide.html#transformer).\n",
        "\n",
        "**Nota:** No modificar el m√©todo set_output de la clase IQR"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3uqK6AZnuhmL"
      },
      "source": [
        "**Respuesta:**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "70CGFkRScKKP"
      },
      "outputs": [],
      "source": [
        "class IQR(BaseEstimator, TransformerMixin):\n",
        "    \"\"\"\n",
        "    Tratamiento de outliers por IQR.\n",
        "    - En fit: calcula Q1, Q3, IQR y l√≠mites inferior/superior por columna num√©rica.\n",
        "    - En transform: aplica clip columna a columna con los l√≠mites aprendidos.\n",
        "    - No elimina filas; solo recorta valores extremos.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, lambda_=1.5):\n",
        "        self.lambda_ = lambda_\n",
        "\n",
        "    def fit(self, X, y=None):\n",
        "        # Convertimos a DataFrame si es necesario\n",
        "        X = X if isinstance(X, pd.DataFrame) else pd.DataFrame(X)\n",
        "\n",
        "        # Seleccionamos solo las columnas num√©ricas\n",
        "        self.columns_ = X.select_dtypes(include=\"number\").columns\n",
        "\n",
        "        # Calculamos Q1, Q3, IQR y l√≠mites\n",
        "        self.q1_ = X[self.columns_].quantile(0.25)\n",
        "        self.q3_ = X[self.columns_].quantile(0.75)\n",
        "        self.iqr_ = self.q3_ - self.q1_\n",
        "\n",
        "        self.lower_ = self.q1_ - self.lambda_ * self.iqr_\n",
        "        self.upper_ = self.q3_ + self.lambda_ * self.iqr_\n",
        "        \n",
        "        # Retornamos el objeto\n",
        "        return self\n",
        "\n",
        "    def transform(self, X):\n",
        "        # Convertimos a DataFrame si es necesario\n",
        "        X = X if isinstance(X, pd.DataFrame) else pd.DataFrame(X)\n",
        "        \n",
        "        # Creamos una copia para no modificar el original\n",
        "        X_out = X.copy()\n",
        "\n",
        "        # Nos aseguramos de trabajar solo con las columnas que existen en X\n",
        "        cols = self.columns_.intersection(X_out.columns)\n",
        "\n",
        "        # Solo aplicamos clip si hay columnas num√©ricas\n",
        "        if len(cols) > 0:\n",
        "            X_out.loc[:, cols] = X_out.loc[:, cols].clip(\n",
        "                lower=self.lower_[cols],\n",
        "                upper=self.upper_[cols],\n",
        "                axis=1\n",
        "            )\n",
        "        \n",
        "        # Retornamos el DataFrame transformado\n",
        "        return X_out\n",
        "\n",
        "    def set_output(self, transform='default'):\n",
        "        # No modificar esta funci√≥n\n",
        "        return self"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Pse94ohOm1um"
      },
      "source": [
        "#### 2.2 Creaci√≥n del Pipeline [0.5 puntos]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JVWWiGA5m_Hj"
      },
      "source": [
        "Para comenzar introduci√©ndose en el uso de pipeline, usted decide definir un pipeline con el Transformer previamente definido. Adem√°s, usted decide visualizar c√≥mo cambia la distribuci√≥n de las variables Precio y Cantidad antes y despues de aplicar IQR. Para ello, usted aplica los siguientes pasos:\n",
        "\n",
        "- Definir un pipeline llamado `numeric_transformations` para las variables precio y cantidad con la transformaci√≥n IQR. [0.1 puntos]\n",
        "- Defina un column transformer que aplique `numeric_transformations` para las variables num√©ricas y `passthrough` para las variables categ√≥ricas. Adicionalmente, fije el par√°metro `verbose_feature_names_out` en `False`. Ver hint al final [0.1 puntos]\n",
        "- Defina el dataframe `df_iqr` aplicado el column transformer a los datos proporcionados por Mr. Cheems considerando un valor de $\\lambda$ que tenga un desempe√±o aceptable para ambas variables. [0.1 puntos]\n",
        "- Usar `explore_data` en `df_retail` y en `df_iqr`.  [0.1 puntos]\n",
        "- Reportar los cambios observados en la distribuci√≥n de las variables. ¬øQu√© sucede al aumentar el valor de lambda? [0.1 puntos]\n",
        "\n",
        "\n",
        "**Hint:** El transformador `passthrough` est√° predefinido y es una opci√≥n que puedes usar para las columnas que no deseas transformar. Al especificar 'passthrough' para una parte de tu ColumnTransformer, las columnas correspondientes pasar√°n a trav√©s del ColumnTransformer sin ninguna modificaci√≥n. El siguiente [enlace](https://scikit-learn.org/stable/modules/generated/sklearn.compose.ColumnTransformer.html) le puede ser √∫til.\n",
        "\n",
        "**Nota:** Mantenga el m√©todo set_output del column transformer con la transformaci√≥n `pandas` para obtener un dataframe una vez aplicado el column transformer."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vkeizZcLuabD"
      },
      "source": [
        "**Respuesta:**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LF24vWb4GwLo"
      },
      "source": [
        "Ap√≥yese de la siguiente estructura para su respuesta:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZaSuz2NSn7g6"
      },
      "outputs": [],
      "source": [
        "# Definicion las variables que pasar√°n por cada pipeline\n",
        "numerical_columns = [\"Price\", \"Quantity\"]\n",
        "categorical_columns = [c for c in df_retail.columns if c not in numerical_columns]\n",
        "\n",
        "# Definicion del pipeline\n",
        "numeric_transformations = Pipeline(steps=[\n",
        "    (\"iqr\", IQR(lambda_=2.0))\n",
        "])\n",
        "\n",
        "# ColumnTransformer\n",
        "column_transformer = ColumnTransformer(\n",
        "    transformers=[\n",
        "        (\"numerical\", numeric_transformations, numerical_columns),\n",
        "        (\"categorical\", \"passthrough\", categorical_columns),\n",
        "    ],\n",
        "    verbose_feature_names_out=False\n",
        ")\n",
        "\n",
        "column_transformer.set_output(transform='pandas')\n",
        "\n",
        "# Aplicamos ColumnTransformer a los datos\n",
        "df_iqr = column_transformer.fit_transform(df_retail)\n",
        "\n",
        "# Gr√°ficos\n",
        "_ = explore_data(df_retail)\n",
        "_ = explore_data(df_iqr)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yPKnc6UcsDkm"
      },
      "source": [
        "*Reporte los cambios observados aqu√≠.*\n",
        "\n",
        "**Antes (df\\_retail)**\n",
        "\n",
        "* Tanto Price como Quantity exhiben colas muy largas hacia la derecha; el eje x se extiende hasta miles (p. ej., >10 000 en Price y >15 000 en Quantity), por lo que toda la masa queda concentrada cerca de 0 y el histograma se ve ‚Äúaplastado‚Äù.\n",
        "* Distribuciones altamente asim√©tricas (sesgo positivo) y con kurtosis elevada (influencia desproporcionada de extremos).\n",
        "* La forma central no es interpretable a simple vista.\n",
        "\n",
        "**Despu√©s (df\\_iqr con `IQR` en Pipeline)**\n",
        "\n",
        "* Se aten√∫an las colas al aplicar $[Q1-\\lambda\\cdot IQR,\\; Q3+\\lambda\\cdot IQR]$ aprendidos en `fit`.\n",
        "  El rango visible queda en valores mucho m√°s razonables (‚âà0‚Äì30 en Quantity y ‚âà0‚Äì9 en Price en la figura), lo que revela la estructura central.\n",
        "* El histograma muestra ahora modas claras y ‚Äúescalones‚Äù esperables (en Quantity se aprecia amontonamiento en enteros; en Price aparecen picos en precios t√≠picos).\n",
        "* No se eliminaron filas: el tama√±o muestral es el mismo; lo que cambia es que los valores extremos quedaron recortados en los umbrales (posible leve ‚Äúacumulaci√≥n‚Äù en los bordes de clip).\n",
        "\n",
        "**Efecto de variar $\\lambda$**\n",
        "\n",
        "* Aumentar $\\lambda$ ‚Üí umbrales m√°s anchos ‚Üí menos recorte, reaparecen colas y la forma se acerca a la original (m√°s asimetr√≠a y varianza).\n",
        "* Disminuir $\\lambda$ ‚Üí umbrales m√°s estrechos ‚Üí recorte m√°s agresivo, mayor concentraci√≥n en la regi√≥n central y picos m√°s marcados en los l√≠mites; riesgo de perder informaci√≥n de la cola si se reduce en exceso.\n",
        "* La elecci√≥n usada (p. ej., $\\lambda \\approx 2$) ofrece buen compromiso para ambas variables: mejora la legibilidad y mantiene se√±ales de negocio (picos en precios/cantidades habituales)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MF5s4dqMYCbJ"
      },
      "source": [
        "### 3. Agregando un imputer al pipeline [1.0 puntos]\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2Bc9fFeXp-At"
      },
      "source": [
        "<p align=\"center\">\n",
        "  <img width=300 src=\"https://media.makeameme.org/created/hmm-there-is.jpg\">\n",
        "</p>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uugEdc26vJ5N"
      },
      "source": [
        "Para continuar con la limpieza del dataframe usted decide imputar los datos nulos de las variables num√©ricas, para lo cual decide realizar las siguientes tareas:\n",
        "\n",
        "1. Crear un pipeline para variables categ√≥ricas llamado `categoric_transformations` con un paso llamado `mode_imputer`, en el cual se imputen los datos faltantes por la categor√≠a m√°s frecuente.\n",
        "2. Agregar al pipeline `numeric_transformations` un paso llamado `mean_imputer`, en el cual se imputen los datos por la media usando [SimpleImputer](https://scikit-learn.org/stable/modules/generated/sklearn.impute.SimpleImputer.html) [0.1 puntos]\n",
        "3. Crear y aplicar un `ColumnTransformer` actualizado con los pipelines `categoric_transformations` y `numeric_transformations` a `df_retail`, creando un dataframe llamado `df_mean_imputer`. [0.1 puntos]\n",
        "4. Comparar los resultados de `explore_data` en `df_mean_imputer` y `df_iqr`. ¬øQu√© diferencias observa en la distribuci√≥n de los datos? [0.2 puntos]\n",
        "5. Cambiar el imputer de `numeric_transformations` por [KNNImputer](https://scikit-learn.org/stable/modules/generated/sklearn.impute.KNNImputer.html) y definir un nuevo dataframe llamado `df_knn_imputer`, aplicando el nuevo ColumnTransformer a `df_retail`. En caso de los tiempos de ejecuci√≥n sean altos puede probar a reducir el par√°metro `n_neighbors`. [0.1 puntos]\n",
        "6. Comparar los resultados de `explore_data` en `df_knn_imputer` y `df_iqr`. ¬øQu√© diferencias observa en la distribuci√≥n de los datos? [0.2 puntos]\n",
        "7. Comparar los resultados de `explore_data` en `df_knn_imputer` y `df_mean_imputer`. ¬øCu√°l m√©todo de imputaci√≥n es mejor? Deje el m√©todo escogido en el ColumnTransformer. [0.2 puntos]\n",
        "\n",
        "**Nota: Fije el par√°metro verbose_feature_names_out en `False` y utilice el m√©todo set_output con transformaci√≥n `pandas` en cada ColumnTransformer para obtener como salida un dataframe.**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ACVUdZZxuo4o"
      },
      "source": [
        "**Respuesta:**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "x8jgag-EYFai"
      },
      "outputs": [],
      "source": [
        "# ---------------------- Partes 1 a 4 ----------------------\n",
        "# Importamos los imputers\n",
        "from sklearn.impute import SimpleImputer, KNNImputer\n",
        "\n",
        "# Definimos las variables que pasar√°n por cada pipeline\n",
        "numerical_columns = [\"Price\", \"Quantity\"]\n",
        "categorical_columns = [c for c in df_retail.columns if c not in numerical_columns]\n",
        "\n",
        "# Agregamos al pipeline numeric_transformations el paso llamado mean_imputer\n",
        "numeric_transformations = Pipeline(steps=[\n",
        "    (\"iqr\", IQR(lambda_=2.0)),\n",
        "    (\"mean_imputer\", SimpleImputer(strategy=\"mean\"))\n",
        "])\n",
        "\n",
        "# Creamos un pipeline para variables categ√≥ricas\n",
        "categoric_transformations = Pipeline(steps=[\n",
        "    (\"mode_imputer\", SimpleImputer(strategy=\"most_frequent\"))\n",
        "])\n",
        "\n",
        "# ColumnTransformer con ambos pipelines\n",
        "column_transformer_mean = ColumnTransformer(\n",
        "    transformers=[\n",
        "        ('numerical', numeric_transformations, numerical_columns),\n",
        "        ('categorical', categoric_transformations, categorical_columns)\n",
        "    ],\n",
        "    verbose_feature_names_out=False\n",
        ")\n",
        "column_transformer_mean.set_output(transform='pandas')\n",
        "\n",
        "# Aplicamos ColumnTransformer a los datos\n",
        "df_mean_imputer = column_transformer_mean.fit_transform(df_retail)\n",
        "\n",
        "# Gr√°ficos\n",
        "_ = explore_data(df_iqr)\n",
        "_ = explore_data(df_mean_imputer)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9PfBHpAsvSuD"
      },
      "source": [
        "*Escriba su respuesta aqu√≠*\n",
        "\n",
        "Comparaci√≥n `df_mean_imputer` vs `df_iqr` (explore\\_data)\n",
        "\n",
        "* En `df_iqr` a√∫n existen `NaN` en Price y Quantity (solo se hab√≠a aplicado el clip IQR).\n",
        "* En `df_mean_imputer` desaparecen los `NaN`; la imputaci√≥n por media introduce acumulaci√≥n artificial en torno al valor medio de cada variable (picos pronunciados cerca de la media). Esto reduce la varianza y distorsiona la forma central, especialmente en distribuciones sesgadas como estas.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ---------------------- Partes 5 a 7 ----------------------\n",
        "# Pipeline con KNNImputer\n",
        "numeric_transformations_knn = Pipeline(steps=[\n",
        "    (\"iqr\", IQR(lambda_=2.0)),\n",
        "    (\"knn_imputer\", KNNImputer(n_neighbors=5, weights=\"distance\"))\n",
        "])\n",
        "\n",
        "# ColumnTransformer con ambos pipelines\n",
        "column_transformer_knn = ColumnTransformer(\n",
        "    transformers=[\n",
        "        ('numerical', numeric_transformations_knn, numerical_columns),\n",
        "        ('categorical', categoric_transformations, categorical_columns)\n",
        "    ],\n",
        "    verbose_feature_names_out=False\n",
        ")\n",
        "column_transformer_knn.set_output(transform='pandas')\n",
        "\n",
        "# Aplicamos ColumnTransformer a los datos\n",
        "df_knn_imputer = column_transformer_knn.fit_transform(df_retail)\n",
        "\n",
        "# Gr√°ficos\n",
        "_ = explore_data(df_iqr)\n",
        "_ = explore_data(df_knn_imputer)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Comparaci√≥n `df_knn_imputer` vs `df_iqr`\n",
        "\n",
        "* Al igual que con la media, desaparecen los `NaN`, pero sin picos artificiales: KNN reparte los valores imputados seg√∫n los vecinos m√°s cercanos en el espacio $Price, Quantity$ (ya ‚Äúsaneado‚Äù por IQR).\n",
        "* La forma de ambas distribuciones se mantiene m√°s realista; la varianza y la asimetr√≠a quedan m√°s alineadas con el patr√≥n observado en los datos no faltantes.\n",
        "\n",
        "Comparaci√≥n `df_knn_imputer` vs `df_mean_imputer` + elecci√≥n del m√©todo\n",
        "\n",
        "* Media: r√°pida, pero aplana la distribuci√≥n alrededor del promedio, reduce correlaciones y puede sesgar indicadores (especialmente con colas largas).\n",
        "* KNN: preserva estructura local y relaciones entre variables; mitiga picos artificiales y mantiene mejor la covarianza.\n",
        "* Dado lo anterior, nos quedamos con KNNImputer."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Dejamos el ColumnTransformer final con KNNImputer\n",
        "column_transformer = column_transformer_knn"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "buuUiW-9YYZ3"
      },
      "source": [
        "### 4. Creaci√≥n de nuevas features [2.0 puntos]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RQSuoL5mubnA"
      },
      "source": [
        "<p align=\"center\">\n",
        "  <img width=250 src=\"https://miro.medium.com/max/1000/1*JtTWgAcfVTWV8OTjT47Atg.jpeg\">\n",
        "</p>\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4-yHP5oIvzFS"
      },
      "source": [
        "#### 4.1 Definicion de LRMFP [1.0 puntos]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qe0V2CnZY8Bc"
      },
      "source": [
        "Dado que Mr. Lepin est√° interesado en obtener nuevos atributos relevantes para su negocio, su equipo de expertos sugiere la construcci√≥n de variables **LRMFP**, las que se construyen en base a las siguientes definiciones:\n",
        "\n",
        "- **Length (L)**: Intervalo de tiempo, en d√≠as, entre la primera y la √∫ltima visita del cliente. Mientras mas grande sea el valor, mas fiel es el cliente.\n",
        "\n",
        "- **Recency (R)**: Indica hace cuanto tiempo el cliente realizo su ultima compra. Notar que para este caso, mientras mas grande es el valor, menos interes posee el usuario para repetir una compra en uno de los locales. **Considere \"hoy\" como la fecha mas reciente del dataset**.\n",
        "\n",
        "- **Monetary (M)**: El t√©rmino \"monetario\" se refiere a la cantidad media de dinero gastada por cada visita del cliente durante el per√≠odo de observaci√≥n y refleja la contribuci√≥n del cliente a los ingresos de la empresa.\n",
        "\n",
        "- **Frequency (F)**: Se refiere al n√∫mero total de visitas del cliente durante el periodo de observaci√≥n. Cuanto mayor sea la frecuencia, mayor ser√° la fidelidad del cliente.\n",
        "\n",
        "- **Periodicity (P)**: Representa si los clientes visitan las tiendas con regularidad.\n",
        "\n",
        "$$Periodicity(n)=std(IVT_1, ..., IVT_n)$$\n",
        "\n",
        "Donde $IVT$ denota el tiempo entre visitas y n representa el n√∫mero de valores de tiempo entre visitas de un cliente.\n",
        "\n",
        "\n",
        "$$IVT_i=date\\_diff(t_{i+1},t)$$\n",
        "\n",
        "En base a las definiciones se√±aladas, dise√±e una funci√≥n que permita obtener las caracter√≠sticas **LRMFP** recibiendo un DataFrame como entrada. Para esto, no estar√° permitido el uso de iteradores, utilice todas las herramientas que les ofrece `pandas` para realizar esto.\n",
        "\n",
        "Una referencia que le puede ser √∫til es el [documento original](https://www.researchgate.net/publication/315979555_LRFMP_model_for_customer_segmentation_in_the_grocery_retail_industry_a_case_study) en donde se propone este m√©todo."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "cell_id": "bee8d549c7c043a5b0cafae0543afadf",
        "deepnote_cell_height": 212.6666717529297,
        "deepnote_cell_type": "markdown",
        "id": "L7ZwWJxhXMfk",
        "tags": []
      },
      "source": [
        "**<u>Formato</u> del Resultado Esperado:**\n",
        "\n",
        "| Customer ID | Length | Recency | Frequency | Monetary | Periodicity |\n",
        "|------------:|-------:|--------:|----------:|---------:|------------:|\n",
        "|   12346.0   |    294 |      67 |        46 |   -64.68 |        37.0 |\n",
        "|   12347.0   |     37 |       3 |        71 |  1323.32 |         0.0 |\n",
        "|   12349.0   |    327 |      43 |       107 |  2646.99 |        78.0 |\n",
        "|   12352.0   |     16 |      11 |        18 |   343.80 |         0.0 |\n",
        "|   12356.0   |     44 |      16 |        84 |  3562.25 |        12.0 |"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "cell_id": "3c7f8a4a06a44cbd8d50e8a4decf4c71",
        "deepnote_cell_height": 52.26666259765625,
        "deepnote_cell_type": "markdown",
        "id": "6GaQZaMXXMfk",
        "tags": []
      },
      "source": [
        "**Respuesta:**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cell_id": "39a8b98eacdc43a4bdfeaa138b746198",
        "deepnote_cell_height": 83.86666870117188,
        "deepnote_cell_type": "code",
        "id": "VsgqgqsjXMfl",
        "owner_user_id": "8c58f50a-7a08-41a2-952e-38bdb7507048",
        "tags": []
      },
      "outputs": [],
      "source": [
        "def custom_features(dataframe_in: pd.DataFrame) -> pd.DataFrame:\n",
        "    \"\"\"\n",
        "    Calcula LRMFP por cliente (vectorizado, sin filtrar filas):\n",
        "      - Length: d√≠as entre primera y √∫ltima visita.\n",
        "      - Recency: d√≠as desde la fecha m√°xima del dataset a la √∫ltima visita.\n",
        "      - Frequency: n√∫mero de visitas (facturas √∫nicas).\n",
        "      - Monetary: gasto promedio por visita (sum(Quantity*Price) por factura).\n",
        "      - Periodicity: std de tiempos entre visitas (en d√≠as), ddof=0.\n",
        "    Devuelve un DataFrame con columnas:\n",
        "    ['Customer ID','Length','Recency','Frequency','Monetary','Periodicity'].\n",
        "    \"\"\"\n",
        "    df = dataframe_in\n",
        "\n",
        "    # Variables m√≠nimas\n",
        "    cols_req = [\"Customer ID\", \"Invoice\", \"InvoiceDate\", \"Quantity\", \"Price\"]\n",
        "    missing = [c for c in cols_req if c not in df.columns]\n",
        "    if missing:\n",
        "        raise ValueError(f\"Faltan columnas requeridas: {missing}\")\n",
        "\n",
        "    # Gasto por l√≠nea (NaN en Quantity/Price se propagan; sum ignora NaN)\n",
        "    line_total = df[\"Quantity\"] * df[\"Price\"]\n",
        "\n",
        "    # Nivel \"visita\": por (Customer ID, Invoice)\n",
        "    visits = (\n",
        "        df.assign(LineTotal=line_total)\n",
        "          .groupby([\"Customer ID\", \"Invoice\"], as_index=False)\n",
        "          .agg(VisitDate=(\"InvoiceDate\", \"max\"),\n",
        "               Spend=(\"LineTotal\", \"sum\"))\n",
        "    )\n",
        "\n",
        "    # Fecha \"hoy\": m√°xima fecha del dataset\n",
        "    end_date = visits[\"VisitDate\"].max()\n",
        "\n",
        "    # F: n√∫mero de visitas\n",
        "    freq = visits.groupby(\"Customer ID\").size().rename(\"Frequency\")\n",
        "\n",
        "    # M: gasto promedio por visita\n",
        "    mon = visits.groupby(\"Customer ID\")[\"Spend\"].mean().rename(\"Monetary\")\n",
        "\n",
        "    # L y R: usando fechas de visita v√°lidas\n",
        "    g_dates = visits.groupby(\"Customer ID\")[\"VisitDate\"]\n",
        "    first = g_dates.min()\n",
        "    last  = g_dates.max()\n",
        "\n",
        "    length = (last - first).dt.days.rename(\"Length\")\n",
        "    recency = (end_date - last).dt.days.rename(\"Recency\")\n",
        "\n",
        "    # P: std de inter-visit times (en d√≠as), ddof=0; clientes con <2 visitas ‚Üí 0\n",
        "    vis_sorted = visits.sort_values([\"Customer ID\", \"VisitDate\"])\n",
        "    ivt_days = vis_sorted.groupby(\"Customer ID\")[\"VisitDate\"].diff().dt.days\n",
        "    periodicity = (\n",
        "        ivt_days.groupby(vis_sorted[\"Customer ID\"])\n",
        "                .std(ddof=0)\n",
        "                .fillna(0.0)\n",
        "                .rename(\"Periodicity\")\n",
        "    )\n",
        "\n",
        "    # Ensamble final\n",
        "    out = (\n",
        "        pd.concat([length, recency, freq, mon, periodicity], axis=1)\n",
        "          .reset_index()\n",
        "          .rename(columns={\"index\": \"Customer ID\"})\n",
        "    )\n",
        "\n",
        "    out[\"Length\"] = out[\"Length\"].fillna(0).astype(\"int64\")\n",
        "    out[\"Recency\"] = out[\"Recency\"].fillna(0).astype(\"int64\")\n",
        "    out[\"Frequency\"] = out[\"Frequency\"].fillna(0).astype(\"int64\")\n",
        "    out[\"Monetary\"] = out[\"Monetary\"].astype(float).round(2)\n",
        "    out[\"Periodicity\"] = out[\"Periodicity\"].astype(float).round(2)\n",
        "\n",
        "    return out[[\"Customer ID\", \"Length\", \"Recency\", \"Frequency\", \"Monetary\", \"Periodicity\"]]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1ddL8wThv36t"
      },
      "source": [
        "#### 4.2 Agregando las custom features [1.0 puntos]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ehLWiQzjwDm-"
      },
      "source": [
        "Ahora, usted decide agregar al pipeline las nuevas variables creadas, para lo cual realiza las siguientes tareas:\n",
        "\n",
        "1. Cree un nuevo pipeline llamado `retail_pipeline` que encapsule el ColumnTransformer y calcule las LRMFP. El primer paso del pipeline ll√°melo  `col_tranformer` y el segundo paso ll√°melo `custom_features`, incorpora las nuevas variables al dataframe. Hint: les puede ser √∫til investigar [este](https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.FunctionTransformer.html) m√©todo. [0.1 puntos]\n",
        "2. Aplicar el pipeline actualizado a los datos proporcionados por Mr. Cheems, creando un nuevo dataframe llamado `df_custom`. [0.1 puntos]\n",
        "3. Explorar la distribuci√≥n de las nuevas variables con `explore_data` y comentar brevemente (2-3 l√≠neas) caracter√≠sticas de cada custom feature. [0.5 puntos]\n",
        "5. Entregar un insight para el negocio en base a las nuevas variables. [0.3 puntos]\n",
        "\n",
        "**Nota:** Recuerde fijar el par√°metro `verbose_feature_names_out` en `False` e incorporar el m√©todo `set_output` para obtener una salida en formato dataframe del ColumnTransformer."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HVCGxPgtwFsk"
      },
      "source": [
        "**Respuesta**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def explore_data_custom(\n",
        "    dataframe_in: pd.DataFrame,\n",
        "    columns=None,\n",
        "    clip_q=(0.0, 0.995),     # para graficar: recorta colas extremas sin filtrar filas\n",
        "    max_bins=60,             # l√≠mite superior de bins\n",
        "):\n",
        "    \"\"\"\n",
        "    Exploraci√≥n configurable para variables continuas:\n",
        "      - Histogramas (Plotly) de las columnas indicadas.\n",
        "      - Conteo de nulos (solo de las columnas analizadas).\n",
        "      - Retorna resumen con stats y cuantiles.\n",
        "\n",
        "    Par√°metros\n",
        "    ----------\n",
        "    dataframe_in : pd.DataFrame\n",
        "    columns : list[str] | None\n",
        "        Columnas a graficar. Si None y existen LRMFP, usa esas; si no, ['Quantity','Price'].\n",
        "    clip_q : (float, float)\n",
        "        Cuantiles [q_low, q_high] para 'clip' con fines de visualizaci√≥n (no modifica el DF).\n",
        "    max_bins : int\n",
        "        M√°ximo de bins permitidos.\n",
        "    \"\"\"\n",
        "    df = dataframe_in\n",
        "\n",
        "    # Selecci√≥n de columnas por defecto\n",
        "    lrmfp = [\"Length\", \"Recency\", \"Frequency\", \"Monetary\", \"Periodicity\"]\n",
        "    if columns is None:\n",
        "        columns = lrmfp if set(lrmfp).issubset(df.columns) else [\"Quantity\", \"Price\"]\n",
        "\n",
        "    # Subconjunto a explorar\n",
        "    sub = df[columns].copy()\n",
        "\n",
        "    # Conteo de nulos de las columnas analizadas\n",
        "    null_counts = sub.isna().sum().sort_values(ascending=False)\n",
        "    print(\"Nulos por columna (subset analizado):\")\n",
        "    print(null_counts)\n",
        "\n",
        "    # Funci√≥n auxiliar: n√∫mero de bins\n",
        "    def _fd_bins(s: pd.Series) -> int:\n",
        "        s = s.dropna().astype(float)\n",
        "        if s.empty:\n",
        "            return 10\n",
        "        q75, q25 = np.percentile(s, [75, 25])\n",
        "        iqr = q75 - q25\n",
        "        if iqr <= 0:\n",
        "            # fallback robusto cuando la IQR es 0\n",
        "            return int(np.clip(int(np.sqrt(s.size)), 10, max_bins))\n",
        "        h = 2.0 * iqr / (np.cbrt(s.size) + 1e-12)\n",
        "        if h <= 0:\n",
        "            return int(np.clip(int(np.sqrt(s.size)), 10, max_bins))\n",
        "        bins = int(np.ceil((s.max() - s.min()) / h))\n",
        "        return int(np.clip(bins, 10, max_bins))\n",
        "\n",
        "    # Graficamos\n",
        "    q_low, q_high = clip_q if clip_q is not None else (None, None)\n",
        "    for col in columns:\n",
        "        s = sub[col].astype(float)\n",
        "\n",
        "        if clip_q is not None:\n",
        "            lo = s.quantile(q_low)\n",
        "            hi = s.quantile(q_high)\n",
        "            s_plot = s.clip(lower=lo, upper=hi)\n",
        "        else:\n",
        "            s_plot = s\n",
        "\n",
        "        nbins = _fd_bins(s_plot)\n",
        "\n",
        "        fig = px.histogram(\n",
        "            s_plot.dropna().to_frame(name=col),\n",
        "            x=col,\n",
        "            nbins=nbins,\n",
        "            title=f\"Distribuci√≥n de {col}\",\n",
        "        )\n",
        "        fig.update_layout(bargap=0.01, xaxis_title=col, yaxis_title=\"count\")\n",
        "        fig.show()\n",
        "\n",
        "    # Resumen estad√≠stico que se retorna\n",
        "    stats = sub.agg([\"count\", \"mean\", \"median\", \"std\", \"min\", \"max\"]).T\n",
        "    qs = sub.quantile([0.01, 0.05, 0.50, 0.95, 0.99]).T\n",
        "    qs.columns = [f\"q{int(q*100):02d}\" for q in qs.columns]\n",
        "    summary = stats.join(qs)\n",
        "\n",
        "    return summary"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def _add_lrmfp(df: pd.DataFrame) -> pd.DataFrame:\n",
        "    # Calcula LRMFP a nivel cliente y las incorpora por merge\n",
        "    feats = custom_features(df)\n",
        "    return df.merge(feats, on=\"Customer ID\", how=\"left\")\n",
        "\n",
        "# Pipeline que encapsula ColumnTransformer + LRMFP\n",
        "retail_pipeline = Pipeline(steps=[\n",
        "    (\"col_tranformer\", column_transformer),                          # ya configurado con verbose_feature_names_out=False\n",
        "    (\"custom_features\", FunctionTransformer(func=_add_lrmfp, validate=False))\n",
        "])\n",
        "retail_pipeline.set_output(transform=\"pandas\")\n",
        "\n",
        "df_custom = retail_pipeline.fit_transform(df_retail)\n",
        "\n",
        "summary_lrmfp = explore_data_custom(\n",
        "    df_custom,\n",
        "    columns=[\"Length\", \"Recency\", \"Frequency\", \"Monetary\", \"Periodicity\"],\n",
        "    clip_q=(0.0, 0.995)  # recorte visual de colas para mejorar lectura\n",
        ")\n",
        "summary_lrmfp"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Respuesta a 3. Caracter√≠sticas de cada custom feature\n",
        "\n",
        "* Length (L): predomina en valores peque√±os (muchos clientes con historia corta o una sola visita), con una cola hacia clientes ‚Äúantiguos‚Äù. Un $L$ alto sugiere fidelidad hist√≥rica, especialmente si va acompa√±ado de $R$ bajo.\n",
        "* Recency (R): asimetr√≠a positiva; masa en valores bajos‚Äìmedios y cola de clientes inactivos (R grande). Clientes con $R$ peque√±o son m√°s ‚Äúcalientes‚Äù para re-compra.\n",
        "* Frequency (F): fuertemente sesgada; mayor√≠a en pocas visitas y pocos con alta repetici√≥n. Es un buen proxy de lealtad al combinarse con $R$ y $L$.\n",
        "* Monetary (M): gasto promedio por visita con cola derecha (tickets altos poco frecuentes). Puede haber valores $\\le 0$ por devoluciones netas; √∫til para identificar perfiles no rentables.\n",
        "* Periodicity (P): gran masa en 0 (un solo ticket o intervalos constantes) y cola hacia arriba (patrones irregulares de visita). Un $P$ bajo indica regularidad, $P$ alto comportamiento err√°tico.\n",
        "\n",
        "#### Repuesta a 4. Insight para el negocio\n",
        "\n",
        "Segmentaci√≥n LRMFP para orquestaci√≥n de campa√±as:\n",
        "\n",
        "* VIP leales: $R$ bajo, $F$ alto, $M$ alto, $P$ bajo ‚Üí mantener con beneficios exclusivos y bundles premium.\n",
        "* En riesgo de abandono: $L$ alto con $R$ alto ‚Üí campa√±as de reactivaci√≥n (recordatorios y descuentos moderados; oferta contextual seg√∫n categor√≠as compradas).\n",
        "* Nuevos/ocasionales: $L=0$ o $F=1$, $P=0$ ‚Üí onboarding con cup√≥n de segunda compra y recomendaciones sencillas.\n",
        "* No rentables / devoluciones: $M<0$ ‚Üí revisar pol√≠ticas de devoluci√≥n, pricing y atenci√≥n para reducir incidencias."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qOV0y-e_lS39"
      },
      "source": [
        "### 5. MinMax Scaler [1.0 puntos]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "T55ZgReXvjGe"
      },
      "source": [
        "<p align=\"center\">\n",
        "  <img width=300 src=\"https://i.imgflip.com/1fsprn.jpg\">\n",
        "</p>\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4dk2R1kvuu-e"
      },
      "source": [
        "#### 5.1 Definici√≥n del Column Transformer [0.5 puntos]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "cell_id": "94c48775ecb4496d970fbd920f65c126",
        "deepnote_cell_height": 268.70001220703125,
        "deepnote_cell_type": "markdown",
        "id": "iWsfp1dKXMfo",
        "tags": []
      },
      "source": [
        "Construya una clase llamada `MinMax()` para realizar una transformaci√≥n de cada una de las columnas de un DataFrame utilizando `ColumnTransformer()`. Recuerde  usar `BaseEstimator` y `TransformerMixin`.\n",
        "\n",
        "\n",
        " Para esto considere que Min-Max escaler queda dada por la ecuaci√≥n:\n",
        "\n",
        "$$MinMax = \\dfrac{x-min(x)}{max(x) - min(x)}$$\n",
        "\n",
        "\n",
        "Consulte el siguiente [link](https://sklearn-template.readthedocs.io/en/latest/user_guide.html#transformer) si tiene dudas sobre la creaci√≥n de custom transformers."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "cell_id": "c087d1fa8aa94d7485fe1292bf628660",
        "deepnote_cell_height": 52.26666259765625,
        "deepnote_cell_type": "markdown",
        "id": "MUOLTWPDXMfo",
        "tags": []
      },
      "source": [
        "**Respuesta:**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cell_id": "07cb4dcf097c4c6baabb9ae2bda25caf",
        "deepnote_cell_height": 83.86666870117188,
        "deepnote_cell_type": "code",
        "id": "g15ZMCs-XMfo",
        "tags": []
      },
      "outputs": [],
      "source": [
        "class MinMax(BaseEstimator, TransformerMixin):\n",
        "\n",
        "    def fit(self,X):\n",
        "        # Convertimos a DataFrame si es necesario\n",
        "        X = X if isinstance(X, pd.DataFrame) else pd.DataFrame(X)\n",
        "        \n",
        "        # Seleccionamos solo las columnas num√©ricas\n",
        "        self.columns_ = X.select_dtypes(include=\"number\").columns\n",
        "        \n",
        "        # Guardamos los m√≠nimos y m√°ximos locales de cada columna num√©rica\n",
        "        self.min_ = X[self.columns_].min()\n",
        "        self.max_ = X[self.columns_].max()\n",
        "        \n",
        "        # Retornamos el objeto\n",
        "        return self\n",
        "\n",
        "    def transform(self,X):\n",
        "        # Convertimos a DataFrame si es necesario\n",
        "        X = X if isinstance(X, pd.DataFrame) else pd.DataFrame(X)\n",
        "        \n",
        "        # Creamos una copia para no modificar el original\n",
        "        X_scaled = X.copy()\n",
        "        \n",
        "        # Aplicamos la transformaci√≥n min-max\n",
        "        X_scaled[self.columns_] = (X[self.columns_] - self.min_) / (self.max_ - self.min_)\n",
        "        \n",
        "        # Retornamos el DataFrame transformado\n",
        "        return X_scaled\n",
        "\n",
        "    def set_output(self,transform='default'):\n",
        "        #No modificar este m√©todo\n",
        "        return self"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RySqWq1Muzp8"
      },
      "source": [
        "#### 5.2 Incorporando MinMax al pipeline [0.5 puntos]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zmIqjkgDwRsV"
      },
      "source": [
        "Ahora, usted decide agregar el escalamiento al pipeline, para lo que decide seguir los siguientes pasos:\n",
        "\n",
        "- Agregar el paso `minmax` al pipeline `numeric_transformations`, haciendo uso de la clase creada. [0.1 puntos]\n",
        "- Defina el dataframe `df_minmax` aplicando el ColumnTransformer actualizado a los datos proporcionados por Mr. Cheems. [0.1 puntos]\n",
        "- Usar `explore_data` en `df_retail` y en `df_minmax`. [0.1 puntos]\n",
        "- Reportar los cambios observados en la distribuci√≥n de las variables.  [0.2 puntos]\n",
        "\n",
        "**Nota:** Recuerde fijar el par√°metro `verbose_feature_names_out` en `False` e incorporar el m√©todo `set_output` para obtener una salida en formato dataframe del ColumnTransformer."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "cell_id": "a480355952a34b6cb7e72afa764091d6",
        "deepnote_cell_height": 52.26666259765625,
        "deepnote_cell_type": "markdown",
        "id": "lL2_CyAGXMfp",
        "tags": []
      },
      "source": [
        "**Respuesta:**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cell_id": "1889976b7a4c40c7825752979b577567",
        "deepnote_cell_height": 65.86666870117188,
        "deepnote_cell_type": "code",
        "id": "NmApXgB8XMfp",
        "tags": []
      },
      "outputs": [],
      "source": [
        "# Obtenemos las columnas num√©ricas del DataFrame\n",
        "numerical_columns = df_retail.select_dtypes(include=\"number\").columns.tolist()\n",
        "\n",
        "# Agregamos el paso minmax al pipeline\n",
        "numeric_transformations = Pipeline(steps=[\n",
        "    (\"iqr\", IQR(lambda_=2.0)),\n",
        "    (\"mean_imputer\", SimpleImputer(strategy=\"mean\")),\n",
        "    (\"minmax\", MinMax())\n",
        "]).set_output(transform='pandas')\n",
        "\n",
        "# ColumnTransformer con el pipeline creado\n",
        "preprocess = ColumnTransformer(\n",
        "    transformers=[\n",
        "        (\"num\", numeric_transformations, numerical_columns)\n",
        "    ],\n",
        "    remainder=\"passthrough\",\n",
        "    verbose_feature_names_out=False\n",
        ").set_output(transform='pandas')\n",
        "\n",
        "# Aplicamos ColumnTransformer a los datos\n",
        "df_minmax = preprocess.fit_transform(df_retail)\n",
        "\n",
        "# Gr√°ficos\n",
        "_ = explore_data(df_retail)\n",
        "_ = explore_data(df_minmax)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Comparaci√≥n `df_minmax` vs `df_iqr`\n",
        "- Ahora todas las variables quedaron reescaladas al rango [0,1].\n",
        "- Los valores m√≠nimos pasaron a 0 y los m√°ximos a 1.\n",
        "- La forma de la distribuci√≥n relativa se preserva, pero ya no est√° dominada por los outliers grandes: se puede apreciar la dispersi√≥n de los datos intermedios.\n",
        "- Los histogramas muestran que:\n",
        "    - Quantity tiene muchos valores acumulados en la parte baja (0‚Äì0.2), algunos ‚Äúpicos‚Äù en rangos medios, y todav√≠a observaciones en el extremo 1 (los outliers originales).\n",
        "    - Price tambi√©n concentra la mayor√≠a de los datos en valores bajos (0‚Äì0.3), pero ahora es posible ver diferentes ‚Äúpicos‚Äù en toda la escala hasta 1, lo que antes estaba oculto."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iWXlAO8-wfNt"
      },
      "source": [
        "### 6. Pregunta te√≥rica [0.5 puntos]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JvsFRwpVtMh_"
      },
      "source": [
        "<p align=\"center\">\n",
        "  <img width=300 src=\"https://file.coinexstatic.com/2023-09-19/166BAC031F222E5910954E7D7D0BC844.png\">\n",
        "</p>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ou7lQIAHwiZv"
      },
      "source": [
        "Finalmente, expl√≠quele a Mr. Cheems porqu√© es √∫til la creaci√≥n de pipelines al momento de hacer Feature Engineering en Machine Learning."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "29QJyzOCwjdD"
      },
      "source": [
        "**Respuesta:**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YMDYYL1stUVO"
      },
      "source": [
        "*Escriba su respuesta aqu√≠*\n",
        "\n",
        "Mr. Cheems, los pipelines son √∫tiles en Feature Engineering pues hay m√∫ltiples pasos que deben ocurrir en un orden exacto (p.ej., tratar outliers ‚Üí imputar ‚Üí transformar por tipo de columna ‚Üí crear variables de negocio). Si se hacen ‚Äúa mano‚Äù, es f√°cil incurrir en inconsistencias entre entrenamiento, validaci√≥n, test o producci√≥n, y tambi√©n en data leakage (usar informaci√≥n del futuro al preparar los datos). Adem√°s, hay otras razones como:\n",
        "\n",
        "* Reproducibles y desplegables: el mismo grafo corre igual en notebook, validaci√≥n y producci√≥n; se versiona y serializa.\n",
        "* Mantenibles y auditables: cambiar hiperpar√°metros (Œª del IQR, vecinos del KNN) o a√±adir pasos no requiere reescribir todo.\n",
        "* Optimizables: permiten usar `GridSearchCV`/`RandomizedSearchCV` sobre el pipeline completo, midiendo el impacto real en desempe√±o.\n",
        "* Heterogeneidad de datos: con `ColumnTransformer` aplicamos tratamientos distintos a num√©ricas y categ√≥ricas y dejamos otras en `passthrough`.\n",
        "* Integran negocio sin fricci√≥n: con `FunctionTransformer` a√±adimos LRMFP de forma determinista y trazable.\n",
        "\n",
        "Resultado para el negocio: procesos de datos consistentes, comparables y sin errores operativos, que aceleran iteraci√≥n y mejoran la confiabilidad de los modelos y las decisiones.\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [
        "wrG4gYabzAHs",
        "MhISwri4zAHy",
        "QDwIXTh7bK_A",
        "Q6nm_0uWvrFv",
        "F4ZY_N0Ad1GP",
        "ECqH4t-Jvj05",
        "Pse94ohOm1um",
        "MF5s4dqMYCbJ",
        "buuUiW-9YYZ3",
        "1ddL8wThv36t",
        "qOV0y-e_lS39",
        "4dk2R1kvuu-e",
        "RySqWq1Muzp8",
        "iWXlAO8-wfNt"
      ],
      "provenance": []
    },
    "deepnote_execution_queue": [],
    "deepnote_notebook_id": "33c253a4f84d40a091bd5023e95abb64",
    "kernelspec": {
      "display_name": "base",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.7"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
